{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install spacy\n",
    "# python -m spacy download en_core_web_sm\n",
    "# pip install --user -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# From Example Code  https://github.com/Swathiu/Detecting-Fake-Reviews/blob/master/Deception_Detection.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, pairwise_distances\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_start_time = time()\n",
    "\n",
    "file_path = \"C:/Users/tsaie/OneDrive/Desktop/FARS (desktop local)/Datasets/\"\n",
    "category = 'Apparel' # Wireless, Tools, Software, Personal_Care_Appliances, Major_Appliances, Mobile_Apps, Mobile_Electronics, PC, Electronics, Automotive, Apparel, Beauty, Office_Products, Outdoors\n",
    "file_name = f'amazon_reviews_us_{category}_v1_00.tsv.gz'\n",
    "\n",
    "# Read in the specified number of rows\n",
    "n_rows_to_read_in = 2_000_000\n",
    "data = pd.read_csv(file_path + file_name, compression='gzip', header=0, sep='\\t', quotechar='\"', error_bad_lines=False, warn_bad_lines=False, nrows=n_rows_to_read_in)\n",
    "\n",
    "# Read in ALL of the rows\n",
    "# data = pd.read_csv(file_path + file_name, compression='gzip', header=0, sep='\\t', quotechar='\"', error_bad_lines=False, warn_bad_lines=False)\n",
    "\n",
    "print(f\"The 'data' file has {data.shape[0]} rows and {data.shape[1]} columns\")\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_small = data[['verified_purchase', 'review_body']]\n",
    "data_small.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Balanced Dataset\n",
    "- have same number of rows of verified and unverified reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def under_sampling(df):\n",
    "    print(\"Under-Sampling Data\")\n",
    "    # Count of Reviews\n",
    "    print(\"Verified:   \", sum(df['verified_purchase'] == 'Y'))\n",
    "    print(\"Un-Verified:\", sum(df['verified_purchase'] == 'N'))\n",
    "\n",
    "    sample_size = sum(df['verified_purchase'] == 'N')\n",
    "    gold_reviews_df = df[df['verified_purchase'] == 'Y']\n",
    "    fake_reviews_df = df[df['verified_purchase'] == 'N']\n",
    "\n",
    "    gold_reviews_us_df = gold_reviews_df.sample(sample_size)\n",
    "    under_sampled_df = pd.concat([gold_reviews_us_df, fake_reviews_df], axis=0)\n",
    "\n",
    "    print(\"Under-Sampled Verified:   \", sum(under_sampled_df['verified_purchase'] == 'Y'))\n",
    "    print(\"Under-Sampled Un-Verified:\", sum(under_sampled_df['verified_purchase'] == 'N'))\n",
    "    \n",
    "\n",
    "    # Graph of Data Distribution\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    sns.countplot(x='verified_purchase', data=under_sampled_df)\n",
    "    plt.title(\"Count of Reviews\")\n",
    "    plt.show()\n",
    "    print(\"Under-Sampling Complete\")\n",
    "    \n",
    "    return under_sampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_equal_weight = under_sampling(data_small)\n",
    "# data_equal_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing Text Reviews\n",
    "def data_cleaning(df):\n",
    "    # Removing emtpy cells\n",
    "    df.dropna(inplace=True)\n",
    "    df['review_cleaned'] = df['review_body'].copy()\n",
    "    \n",
    "    # Removing Unicode Chars (URL)\n",
    "    df['review_cleaned'] = df['review_cleaned'].apply(\n",
    "        lambda rev: re.sub(r\"(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", rev))\n",
    "        \n",
    "    # Replace HTML keywords with blank space (\"&quot;\", \"br\", \"&#34\")\n",
    "    remove_dict = {\"<br /><br />\": \" \", \"<br />\": \" \", \"br \": \"\", \"&quot;\": \" \", \"&#34\": \" \",\n",
    "                   \"<BR>\": \" \", \"_\": \"\"}\n",
    "    for key, val in remove_dict.items():\n",
    "        df['review_cleaned'] = df['review_cleaned'].apply(\n",
    "            lambda x: x.replace(key, val))\n",
    "        \n",
    "    print(\"\\n######## Remove URL and HTML Keywords Complete ########\")\n",
    "    \n",
    "    # Remove Punctuations and numbers\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    df['review_cleaned'] = df['review_cleaned'].apply(\n",
    "        lambda x: ' '.join([word for word in tokenizer.tokenize(x)]))\n",
    "    \n",
    "    remove_dict = {\"0\": \"\", \"1\": \"\", \"2\": \"\", \"3\": \"\", \"4\": \"\", \"5\": \"\", \"6\": \"\", \"7\": \"\", \"8\": \"\", \"9\": \"\",\n",
    "                   \"(\": \"\", \")\":\"\"}\n",
    "    for key, val in remove_dict.items():\n",
    "        df['review_cleaned'] = df['review_cleaned'].apply(\n",
    "            lambda x: x.replace(key, val))\n",
    "    \n",
    "    print(\"\\n######## Remove Punctuation and Numbers Complete ########\")\n",
    "    \n",
    "    # Lowercase Words\n",
    "    df['review_cleaned'] = df['review_cleaned'].str.lower()\n",
    "    \n",
    "    print(\"\\n######## Lowercase Complete ########\")\n",
    "\n",
    "    # Remove Stop Words.\n",
    "    stop = stopwords.words('english')\n",
    "      \n",
    "    df['review_cleaned'] = df['review_cleaned'].apply(\n",
    "        lambda x: ' '.join([word for word in x.split() if word.strip() not in stop]))\n",
    "    \n",
    "    print(\"\\n######## Remove Stop Words Complete ########\")\n",
    "    \n",
    "    # Lemmatization using .lemma_\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "    df['review_cleaned'] = df['review_cleaned'].apply(\n",
    "        lambda x: ' '.join([token.lemma_ for token in nlp(x)]))\n",
    "    \n",
    "    print(\"\\n######## Data Cleaning Complete ########\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Clean the dataset\n",
    "data_cleaned = data_cleaning(data_equal_weight)\n",
    "data_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering + Prepare Data for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/48331315/how-to-extract-all-the-ngrams-from-a-text-dataframe-column-in-different-order-in\n",
    "\n",
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "from itertools import chain\n",
    "\n",
    "def find_ngrams(input_list, n):\n",
    "    return list(zip(*[input_list[i:] for i in range(n)]))\n",
    "\n",
    "def add_bigram_column(df):\n",
    "    copy = df.copy()\n",
    "    copy['bigrams'] = copy['review_cleaned'].map(lambda x: find_ngrams(x.split(), 2))\n",
    "    return copy\n",
    "    \n",
    "data_cleaned = add_bigram_column(data_cleaned)\n",
    "data_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's call the bigrams in vertified reviews \"gold_bigrams\" and the bigrams in unverified reviews \"fake_bigrams\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fake = data_cleaned[data_cleaned['verified_purchase'] == 'N']\n",
    "data_gold = data_cleaned[data_cleaned['verified_purchase'] == 'Y']\n",
    "\n",
    "data_fake.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataframe into data_for_bigram and data_for_ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake = data_fake.sample(frac = 0.50)\n",
    "data_for_ML_fake = data_fake.drop(df_fake.index)\n",
    "\n",
    "df_gold = data_gold.sample(frac = 0.50)\n",
    "data_for_ML_gold = data_gold.drop(df_gold.index)\n",
    "\n",
    "data_cleaned = pd.concat([data_for_ML_fake, data_for_ML_gold], ignore_index=True)\n",
    "\n",
    "print(f'df_fake has {len(df_fake)} rows.')\n",
    "print(f'df_gold has {len(df_gold)} rows.')\n",
    "print(f'data_cleaned has {len(data_cleaned)} rows.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gold_bigrams(df_gold):\n",
    "    global gold_bigrams, gold_bigram_counts\n",
    "    gold_bigrams = df_gold['bigrams'].tolist()\n",
    "    gold_bigrams = list(chain(*gold_bigrams))\n",
    "    gold_bigram_counts = Counter(gold_bigrams)\n",
    "\n",
    "create_gold_bigrams(df_gold)\n",
    "print(f'gold_bigram_counts has {len(gold_bigram_counts)} items')\n",
    "gold_bigram_counts.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fake_bigrams(df_fake):\n",
    "    global fake_bigrams, fake_bigram_counts\n",
    "    fake_bigrams = df_fake['bigrams'].tolist()\n",
    "    fake_bigrams = list(chain(*fake_bigrams))\n",
    "    fake_bigram_counts = Counter(fake_bigrams)\n",
    "    \n",
    "create_fake_bigrams(df_fake)\n",
    "print(f'fake_bigram_counts has {len(fake_bigram_counts)} items')\n",
    "fake_bigram_counts.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_file_path = \"C:/Users/tsaie/OneDrive/Desktop/FARS/Ester Tsai (Bigram and ML)/Bigrams/\"\n",
    "\n",
    "gold_bigram_counts_df = pd.DataFrame(gold_bigram_counts.most_common(), columns=['bigram','count'])\n",
    "gold_bigram_file_name = f'{category} gold_bigrams.csv'\n",
    "gold_bigram_counts_df.to_csv(bigram_file_path + gold_bigram_file_name)\n",
    "\n",
    "fake_bigram_counts_df = pd.DataFrame(fake_bigram_counts.most_common(), columns=['bigram','count'])\n",
    "fake_bigram_file_name = f'{category} fake_bigrams.csv'\n",
    "fake_bigram_counts_df.to_csv(bigram_file_path + fake_bigram_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Save Data Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_counter_N_most_common(gold_or_fake, N):\n",
    "    counter = eval(f'{gold_or_fake}_bigram_counts')\n",
    "    d = {}\n",
    "    for tup in counter.most_common(N):\n",
    "        bigram = f\"{tup[0][0]} {tup[0][1]}\"\n",
    "        d[bigram] = tup[1]\n",
    "    \n",
    "    df = pd.Series(data=d).sort_values(ascending=False).to_frame().reset_index().rename(columns={'index': 'bigram', 0: 'count'})\n",
    "    f, ax = plt.subplots(figsize=(8, N/4 + 1))\n",
    "    if gold_or_fake == 'gold':\n",
    "        palette = sns.color_palette('flare', n_colors=N)\n",
    "    else:\n",
    "        palette = sns.color_palette('crest', n_colors=N)\n",
    "    palette.reverse()\n",
    "    title = f'Top {N} Bigrams in {gold_or_fake.capitalize()} Reviews for the {category} Category'\n",
    "    sns.barplot(x='count', y='bigram', data=df, orient='h', palette=palette).set_title(title)\n",
    "    \n",
    "    file_path = f\"C:/Users/tsaie/OneDrive/Desktop/FARS/Ester Tsai (Bigram and ML)/Images/{category}/\"\n",
    "    if not os.path.exists(file_path):\n",
    "        os.makedirs(file_path)\n",
    "    plt.savefig(file_path + f'{title}.png', bbox_inches='tight', transparent=True)\n",
    "\n",
    "plot_counter_N_most_common(\"gold\", 15)\n",
    "plot_counter_N_most_common(\"fake\", 15)\n",
    "plot_counter_N_most_common(\"gold\", 20)\n",
    "plot_counter_N_most_common(\"fake\", 20)\n",
    "plot_counter_N_most_common(\"gold\", 30)\n",
    "plot_counter_N_most_common(\"fake\", 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "def wordcloud_N_most_common(gold_or_fake, N):\n",
    "    counter = eval(f'{gold_or_fake}_bigram_counts')\n",
    "    d = {}\n",
    "    for tup in counter.most_common(N):\n",
    "        bigram = f\"{tup[0][0]} {tup[0][1]}\"\n",
    "        d[bigram] = tup[1]\n",
    "       \n",
    "    colors = 'summer' if gold_or_fake == 'fake' else 'magma'\n",
    "    word_cloud = WordCloud(width=3000, height=1500, background_color=None, mode = \"RGBA\", colormap=colors)\n",
    "    word_cloud.generate_from_frequencies(frequencies=d)\n",
    "\n",
    "    plt.figure(figsize=(14,7))\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.imshow(word_cloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    title = f'WORDCLOUD - Most Popular Bigrams in {gold_or_fake.capitalize()} Reviews for the {category} Category'\n",
    "    plt.title(title, fontsize=15)\n",
    "\n",
    "    file_path = f\"C:/Users/tsaie/OneDrive/Desktop/FARS/Ester Tsai (Bigram and ML)/Images/{category}/\"    \n",
    "    if not os.path.exists(file_path):\n",
    "        os.makedirs(file_path)\n",
    "    plt.savefig(file_path + f'{title}.png', bbox_inches='tight', transparent=True)\n",
    "   \n",
    "    plt.show()\n",
    "   \n",
    "wordcloud_N_most_common('gold', 150)\n",
    "wordcloud_N_most_common('fake', 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Features\n",
    "\n",
    "**count** = number of bigrams in a review\n",
    "\n",
    "**percent** = number of gold/fake_bigrams as a percentage of total number of bigrams in a review.\n",
    "\n",
    "**simple score** = sum of the gold/fake_bigrams' popularity scores (calculated using the bigram's count in the Counter)\n",
    "\n",
    "**normalized score** = simple score / total bigram count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_bigram_count(bigrams, bigram_dict):\n",
    "    if len(bigrams) == 0:\n",
    "        return 0\n",
    "    \n",
    "    count = 0\n",
    "    for bigram in bigrams:\n",
    "        if bigram in bigram_dict.keys():\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def get_count_percent(bigrams, bigram_dict):\n",
    "    if len(bigrams) == 0:\n",
    "        return 0\n",
    "    \n",
    "    count = get_bigram_count(bigrams, bigram_dict)\n",
    "    return count / len(bigrams)\n",
    "\n",
    "def get_averge_top_score(bigrams, bigram_dict, topN=5):\n",
    "    if len(bigrams) == 0:\n",
    "        return 0\n",
    "    \n",
    "    scores = np.array([])\n",
    "    for bigram in bigrams:\n",
    "        if bigram in bigram_dict.keys():\n",
    "            scores = np.append(scores, bigram_dict[bigram])\n",
    "    \n",
    "    if len(bigrams) <= topN:\n",
    "        return scores.mean()\n",
    "    \n",
    "    sort_descending = -np.sort(-scores)[:topN]\n",
    "    avg_top_score = sort_descending.mean()\n",
    "    return avg_top_score\n",
    "\n",
    "def get_normalized_score(bigrams, bigram_dict):\n",
    "    if len(bigrams) == 0:\n",
    "        return 0\n",
    "    \n",
    "    score = 0\n",
    "    for bigram in bigrams:\n",
    "        if bigram in bigram_dict.keys():\n",
    "            score += bigram_dict[bigram]\n",
    "    return score / len(bigrams)\n",
    "\n",
    "def get_unique_percent(bigrams, bigram_dict, the_other_bigram_dict, unique_threshold=0):\n",
    "    # Count the number of bigrams in a review that appear in bigram_dict but not the_other_bigram_dict. \n",
    "    # Can adjust unique_threshold so you can count also the bigrams that appear in ...\n",
    "    # ...the_other_bigram_dict fewer than unique_threshold times\n",
    "    \n",
    "    if len(bigrams) == 0:\n",
    "        return 0\n",
    "    \n",
    "    count = get_bigram_count(bigrams, bigram_dict)\n",
    "    for bigram in bigrams:\n",
    "        if bigram in the_other_bigram_dict.keys():\n",
    "            if the_other_bigram_dict[bigram] > unique_threshold:\n",
    "                count -= 1\n",
    "    return count / len(bigrams)\n",
    "\n",
    "def has_fake_keywords(bigrams):\n",
    "    fake_keywords = ['honest', 'unbiased', 'unbias', 'biased', 'bias', 'neutral', 'impartial', 'truthful', \n",
    "                     'discount', 'free', 'promotion', 'promote', 'complimentary', 'test', 'influence', 'influencer',\n",
    "                     'independent']\n",
    "    fake_tuples = [('receive', 'product'), ('product', 'receive'), ('provide', 'review'), \n",
    "                   ('product', 'exchange'), ('exchange', 'review'), ('review', 'opinion'),\n",
    "                   ('sample', 'provide'), ('provide', 'sample'), ('sample', 'review'), ('review', 'sample'), \n",
    "                   ('sample', 'product'), ('supply', 'sample'), ('receive', 'sample'), ('sample', 'receive')] \n",
    "    \n",
    "    for kw in fake_tuples:\n",
    "        if kw in bigrams:\n",
    "            return True\n",
    "            \n",
    "    for bigram in bigrams:\n",
    "        for kw in fake_keywords:\n",
    "            if kw in bigram:\n",
    "                return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "# Add bigram_count\n",
    "data_cleaned['bigram_count'] = data_cleaned['bigrams'].apply(lambda x: len(x))\n",
    "\n",
    "# Add features based on gold or fake bigrams\n",
    "fake_bigram_dict = dict(fake_bigram_counts)\n",
    "fake_bigram_dict_filtered = dict((k, v) for k, v in fake_bigram_dict.items() if v >= 2)\n",
    "\n",
    "gold_bigram_dict = dict(gold_bigram_counts)\n",
    "gold_bigram_dict_filtered = dict((k, v) for k, v in gold_bigram_dict.items() if v >= 2)\n",
    "\n",
    "\n",
    "def add_gold_fake_features(df):\n",
    "    \n",
    "    for gold_or_fake in ['gold', 'fake']:\n",
    "\n",
    "        exec(f\"df['{gold_or_fake}%'] = df['bigrams'].apply(\\\n",
    "            lambda x: get_count_percent(x, {gold_or_fake}_bigram_dict_filtered))\")\n",
    "        \n",
    "        exec(f\"df['{gold_or_fake}_unique%'] = df['bigrams'].apply(\\\n",
    "            lambda x: get_unique_percent(x, {gold_or_fake}_bigram_dict, {gold_or_fake}_bigram_dict, 3))\")\n",
    "\n",
    "        exec(f\"df['{gold_or_fake}_score'] = df['bigrams'].apply(\\\n",
    "            lambda x: get_normalized_score(x, {gold_or_fake}_bigram_dict))\")\n",
    "        \n",
    "        exec(f\"df['{gold_or_fake}_top_score'] = df['bigrams'].apply(\\\n",
    "            lambda x: get_averge_top_score(x, {gold_or_fake}_bigram_dict, 1))\")\n",
    "        \n",
    "        exec(f\"df['{gold_or_fake}_top_avg_score'] = df['bigrams'].apply(\\\n",
    "            lambda x: get_averge_top_score(x, {gold_or_fake}_bigram_dict, 5))\")\n",
    "\n",
    "    df['has_fake_keywords'] = df['bigrams'].apply(lambda x: has_fake_keywords(x))\n",
    "    \n",
    "    return df\n",
    "\n",
    "data_cleaned = add_gold_fake_features(data_cleaned).fillna(0)\n",
    "\n",
    "data_cleaned.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Use Machine Learning to Make Predictions for Verified VS. Unverified\n",
    "LABELS: \n",
    "- 1 = verified review\n",
    "- 0 = unverified review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['bigram_count', \n",
    "            'fake%', 'fake_unique%', 'fake_score', 'fake_top_avg_score', 'fake_top_score',\n",
    "            'gold%', 'gold_unique%', 'gold_score', 'gold_top_avg_score', 'gold_top_score',\n",
    "            'has_fake_keywords']\n",
    "\n",
    "'''\n",
    "['bigram_count', \n",
    "            'fake%', 'fake_unique%', 'fake_score', 'fake_top_avg_score', 'fake_top_score',\n",
    "            'gold%', 'gold_unique%', 'gold_score', 'gold_top_avg_score', 'gold_top_score',\n",
    "            'has_fake_keywords']\n",
    "'''\n",
    "\n",
    "def semi_supervised_learning(df, model, algorithm, threshold=0.8, iterations=40):\n",
    "    df = df.copy()\n",
    "    \n",
    "    df_unlabled = df[features]\n",
    "\n",
    "\n",
    "    df['verified_purchase'] = df['verified_purchase'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "    print(\"Training \" + algorithm + \" Model\")\n",
    "    labels = df['verified_purchase']\n",
    "    \n",
    "    train_data, test_data, train_label, test_label = train_test_split(df_unlabled, labels, test_size=0.25, random_state=42)\n",
    "\n",
    "    test_data_copy = test_data.copy()\n",
    "    test_label_copy = test_label.copy()\n",
    "    \n",
    "    all_labeled = False\n",
    "\n",
    "    current_iteration = 0\n",
    "\n",
    "    pbar = tqdm(total=iterations)\n",
    "\n",
    "    while not all_labeled and (current_iteration < iterations):\n",
    "        current_iteration += 1\n",
    "        model.fit(train_data, train_label)\n",
    "\n",
    "        probabilities = model.predict_proba(test_data)\n",
    "        pseudo_labels = model.predict(test_data)\n",
    "\n",
    "        indices = np.argwhere(probabilities > threshold)\n",
    "\n",
    "        for item in indices:\n",
    "            train_data.loc[test_data.index[item[0]]] = test_data.iloc[item[0]]\n",
    "            train_label.loc[test_data.index[item[0]]] = pseudo_labels[item[0]]\n",
    "        test_data.drop(test_data.index[indices[:, 0]], inplace=True)\n",
    "        test_label.drop(test_label.index[indices[:, 0]], inplace=True)\n",
    "\n",
    "        print(\"--\" * 20)\n",
    "\n",
    "        if len(test_data) == 0:\n",
    "            print(\"Exiting loop\")\n",
    "            all_labeled = True\n",
    "        pbar.update(1)\n",
    "        \n",
    "    pbar.close()\n",
    "    predicted_labels = model.predict(test_data_copy)\n",
    "    \n",
    "    # RECORD TEST RESULTS\n",
    "    test_df = test_data_copy.copy()\n",
    "    test_df['predicted_label'] = predicted_labels\n",
    "    test_df['true_label'] = list(test_label_copy)\n",
    "    \n",
    "    \n",
    "    print(algorithm + ' Model Results')\n",
    "    print('--' * 20)\n",
    "    accuracy = accuracy_score(test_label_copy, predicted_labels)\n",
    "    print('Accuracy Score : ' + str(accuracy))\n",
    "    print('Precision Score : ' + str(precision_score(test_label_copy, predicted_labels, pos_label=1)))\n",
    "    print('Recall Score : ' + str(recall_score(test_label_copy, predicted_labels, pos_label=1)))\n",
    "    print('F1 Score : ' + str(f1_score(test_label_copy, predicted_labels, pos_label=1)))\n",
    "#     print('Confusion Matrix : \\n' + str(confusion_matrix(test_label_copy, predicted_labels)))\n",
    "    plot_confusion_matrix(test_label_copy, predicted_labels, classes=[0, 1],\n",
    "                          title=algorithm + ' Confusion Matrix').show()\n",
    "    \n",
    "    return model, test_df, round(accuracy, 4)\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes, title=None, cmap=plt.cm.Blues):\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           xticklabels=classes,\n",
    "           yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Method #1: Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start_time = time()\n",
    "# rf = RandomForestClassifier(random_state=42, criterion='entropy', max_depth=14, max_features='auto', n_estimators=500)\n",
    "rf = RandomForestClassifier(random_state=42, criterion='entropy', max_depth=16, max_features='auto', n_estimators=500)\n",
    "rf_model, test_df, accuracy = semi_supervised_learning(data_cleaned, model=rf, threshold=0.7, iterations=10, algorithm='Random Forest')\n",
    "end_time = time()\n",
    "\n",
    "print(\"Time taken : \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_0 = test_df[test_df['true_label'] == 0]\n",
    "true_1 = test_df[test_df['true_label'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(df):\n",
    "    gold_stat = round(df['gold_unique%'].mean(), 3)\n",
    "    print(f\"gold_stat {gold_stat}\")\n",
    "    \n",
    "    fake_stat = round(df['fake_unique%'].median(), 3)\n",
    "    print(f\"fake_stat {fake_stat}\")\n",
    "    \n",
    "print('true_1 (gold)\\n')\n",
    "get_stats(true_1)\n",
    "\n",
    "print('\\ntrue_0 (fake)\\n')\n",
    "get_stats(true_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond = test_df[(test_df['has_fake_keywords'])]\n",
    "size = cond.shape[0]\n",
    "stat_1 = round(cond['true_label'].mean(), 3)\n",
    "stat_0 = round(1 - stat_1, 3)\n",
    "print(f\"gold percentage = {stat_1}  ({round(stat_1 * size)} / {size})\")\n",
    "print(f\"fake percentage = {stat_0}  ({round(stat_0 * size)} / {size})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 'gold_score'\n",
    "y = 'fake_score'\n",
    "x1, y1 = true_1[x], true_1[y]\n",
    "x2, y2 = true_0[x], true_0[y]\n",
    "fig = plt.figure(figsize=(18, 10))\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "ax1.scatter(x2, y2, s=3, alpha=0.5, c='r', marker=\"o\", label='fake')\n",
    "ax1.scatter(x1, y1, s=3, alpha=0.5, c='b', marker=\"o\", label='gold')\n",
    "# ax1.scatter(x2, y2, s=3, alpha=0.2, c='r', marker=\"o\", label='fake')\n",
    "plt.xlabel(x, fontsize=15)\n",
    "plt.ylabel(y, fontsize=15)\n",
    "plt.legend(loc='upper left', fontsize=15);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = test_df[['true_label', 'predicted_label']].merge(data_cleaned, how='left', left_index=True, right_index=True)\n",
    "ML_test_result_file_path = \"C:/Users/tsaie/OneDrive/Desktop/FARS/Ester Tsai (Bigram and ML)/ML Test Result (CSV)/\"\n",
    "ML_test_result_file_name = f'2022_05_11 {(category)} Test Result.csv'\n",
    "test_result.to_csv(ML_test_result_file_path + ML_test_result_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_result[(test_result['true_label'] == 0) & (test_result['predicted_label'] == 1)].sample(3).drop(columns=['predicted_label','true_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = test_result[(test_result['has_fake_keywords'])]\n",
    "z[z['true_label'] == 0].sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = 'honest'\n",
    "N = min(len(gold_bigram_counts), len(fake_bigram_counts))\n",
    "print(f'fake | {sum([keyword in tup for tup in dict(fake_bigram_counts.most_common(N)).keys()])}')\n",
    "print(f'gold | {sum([keyword in tup for tup in dict(gold_bigram_counts.most_common(N)).keys()])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "ML_model_file_path = \"C:/Users/tsaie/OneDrive/Desktop/FARS/Ester Tsai (Bigram and ML)/ML Models/\"\n",
    "ML_model_file_name = f'2022_05_11 rf_model {(category)}.joblib'\n",
    "dump(rf_model, ML_model_file_path + ML_model_file_name, compress=9)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_end_time = time()\n",
    "print(round(overall_end_time - overall_start_time), \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEXT STEP: Making Predictions for a User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML_model_file_path = \"C:/Users/tsaie/OneDrive/Desktop/FARS/Ester Tsai (Bigram and ML)/ML Models/\"\n",
    "# ML_model_file_name = f'2022_05_05 rf_model {(category)}.joblib'\n",
    "rf_model = load(ML_model_file_path + ML_model_file_name) \n",
    "\n",
    "bigram_file_path = \"C:/Users/tsaie/OneDrive/Desktop/FARS/Ester Tsai (Bigram and ML)/Bigrams/\"\n",
    "\n",
    "gold_bigram_file_name = f'{(category)} gold_bigrams.csv'\n",
    "gold_bigram_df = pd.read_csv(bigram_file_path + gold_bigram_file_name, index_col=0)\n",
    "display(gold_bigram_df.head(3))\n",
    "\n",
    "fake_bigram_file_name = f'{(category)} fake_bigrams.csv'\n",
    "fake_bigram_df = pd.read_csv(bigram_file_path + fake_bigram_file_name, index_col=0)\n",
    "display(fake_bigram_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gold_bigram_df_to_vars(gold_bigram_df):\n",
    "    global gold_bigrams, gold_bigram_dict, gold_bigram_dict_filtered\n",
    "    gold_bigrams = gold_bigram_df['bigram'].to_list()\n",
    "    str_to_tuple = lambda x: (x.split(\"'\")[1], x.split(\"'\")[3])\n",
    "    gold_bigrams = list(map(str_to_tuple, gold_bigrams))\n",
    "    gold_bigram_dict = {gold_bigrams[i]: gold_bigram_df['count'].iloc[i] for i in range(len(gold_bigrams))}\n",
    "    gold_bigram_dict_filtered = dict((k, v) for k, v in gold_bigram_dict.items() if v >= 2)\n",
    "    \n",
    "def fake_bigram_df_to_vars(fake_bigram_df):\n",
    "    global fake_bigrams, fake_bigram_dict, fake_bigram_dict_filtered\n",
    "    fake_bigrams = fake_bigram_df['bigram'].to_list()\n",
    "    str_to_tuple = lambda x: (x.split(\"'\")[1], x.split(\"'\")[3])\n",
    "    fake_bigrams = list(map(str_to_tuple, fake_bigrams))\n",
    "    fake_bigram_dict = {fake_bigrams[i]: fake_bigram_df['count'].iloc[i] for i in range(len(fake_bigrams))}\n",
    "    fake_bigram_dict_filtered = dict((k, v) for k, v in fake_bigram_dict.items() if v >= 2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def clean_review_and_add_features(review, gold_bigram_df, fake_bigram_df):\n",
    "    # Create dataframe for the singular review\n",
    "    df = pd.DataFrame(data={'review_body': review}, index=[0])\n",
    "    \n",
    "    # Generate gold/{gold_or_fake} bigrams in order to score the user input\n",
    "    gold_bigram_df_to_vars(gold_bigram_df)\n",
    "    fake_bigram_df_to_vars(fake_bigram_df)\n",
    "    \n",
    "    # Clean the user input\n",
    "    df = data_cleaning(df) \n",
    "#     df = add_bigram_column(df)\n",
    "    df['bigrams'] = df['review_cleaned'].map(lambda x: find_ngrams(x.split(), 2))\n",
    "    \n",
    "    # Add features\n",
    "    df['bigram_count'] = df['bigrams'].apply(lambda x: len(x))\n",
    "    \n",
    "    df = add_gold_fake_features(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try changing the review and see what the RandomForest model predicts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = \"Turbo charger, small and portable and fits perfectly in your pocket! A nice boost over regular chargers. I received this in exchange for an unbiased review.\"\n",
    "#9994 \"This is a thick high quality cable, my picture is crystal clear and this is just the right length for my needs, I received this in exchange for my honest review.\"\n",
    "#112 Small,lightweight,fits on my desk. Does not come with batteries,I already had some.I received a discount for this review.\n",
    "#59356 Turbo charger, small and portable and fits perfectly in your pocket! A nice boost over regular chargers. I received this in exchange for an unbiased review.\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "# Clean the user input and create a dataframe for it\n",
    "user_input_processed_df = clean_review_and_add_features(review, gold_bigram_df, fake_bigram_df)\n",
    "display(user_input_processed_df)\n",
    "\n",
    "def prepare_df_for_prediction(processed_df):\n",
    "\n",
    "    return processed_df[features]\n",
    "\n",
    "df_for_prediction = prepare_df_for_prediction(user_input_processed_df)\n",
    "\n",
    "prediction, probabilities = rf_model.predict(df_for_prediction), rf_model.predict_proba(df_for_prediction)[0]\n",
    "\n",
    "def interpret_prediction(review, pred, proba):\n",
    "    prob_unverified, prob_verified = round(proba[0] * 100, 1), round(proba[1] * 100, 1)\n",
    "    print(f'\\nREVIEW: \"{review}\" \\n')\n",
    "    if pred == 1:\n",
    "        print(f'PREDICTION: VERIFIED ({prob_verified}%) | UNVERIFIED ({prob_unverified}%)')\n",
    "    if pred == 0:\n",
    "        print(f'PREDICTION: UNVERIFIED ({prob_unverified}%) | VERIFIED ({prob_verified}%)')\n",
    "\n",
    "end_time = time()\n",
    "# print(round(end_time - start_time), \"seconds\")\n",
    "\n",
    "interpret_prediction(review, prediction[0], probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

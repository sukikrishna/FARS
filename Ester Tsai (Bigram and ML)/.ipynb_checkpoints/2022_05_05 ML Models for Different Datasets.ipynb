{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# From Example Code  https://github.com/Swathiu/Detecting-Fake-Reviews/blob/master/Deception_Detection.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, pairwise_distances\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_start_time = time()\n",
    "\n",
    "file_path = \"C:/Users/tsaie/OneDrive/Desktop/000 Resumes & Projects/# Projects/DS3 Fake Amazon Reviews/Dataset/\"\n",
    "\n",
    "category = 'Electronics' # Wireless, Tools, Software, Personal_Care_Appliances, Major_Appliances, Mobile_Apps, Mobile_Electronics, PC, Electronics, Automotive, Apparel, Beauty, Office_Products, Outdoors\n",
    "file_name = f'amazon_reviews_us_{category}_v1_00.tsv.gz'\n",
    "\n",
    "# Read in the specified number of rows\n",
    "n_rows_to_read_in = 500_000\n",
    "data = pd.read_csv(file_path + file_name, compression='gzip', header=0, sep='\\t', quotechar='\"', error_bad_lines=False, warn_bad_lines=False, nrows=n_rows_to_read_in)\n",
    "\n",
    "# Read in ALL of the rows\n",
    "# data = pd.read_csv(file_path + file_name, compression='gzip', header=0, sep='\\t', quotechar='\"', error_bad_lines=False, warn_bad_lines=False)\n",
    "\n",
    "print(f\"The 'data' file has {data.shape[0]} rows and {data.shape[1]} columns\")\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_small = data[['verified_purchase', 'review_body']]\n",
    "data_small.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Balanced Dataset\n",
    "- have same number of rows of verified and unverified reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def under_sampling(df):\n",
    "    print(\"Under-Sampling Data\")\n",
    "    # Count of Reviews\n",
    "    print(\"Verified:\", sum(df['verified_purchase'] == 'Y'))\n",
    "    print(\"Un-Verified:\", sum(df['verified_purchase'] == 'N'))\n",
    "\n",
    "    sample_size = sum(df['verified_purchase'] == 'N')\n",
    "\n",
    "    authentic_reviews_df = df[df['verified_purchase'] == 'Y']\n",
    "    fake_reviews_df = df[df['verified_purchase'] == 'N']\n",
    "\n",
    "    authentic_reviews_us_df = authentic_reviews_df.sample(sample_size)\n",
    "    under_sampled_df = pd.concat([authentic_reviews_us_df, fake_reviews_df], axis=0)\n",
    "\n",
    "    print(\"Under-Sampled Verified\", sum(under_sampled_df['verified_purchase'] == 'Y'))\n",
    "    print(\"Under-Sampled Un-Verified\", sum(under_sampled_df['verified_purchase'] == 'N'))\n",
    "    \n",
    "\n",
    "    # Graph of Data Distribution\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    sns.countplot(x='verified_purchase', data=under_sampled_df)\n",
    "    plt.title(\"Count of Reviews\")\n",
    "    plt.show()\n",
    "    print(\"Under-Sampling Complete\")\n",
    "    return under_sampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_equal_weight = under_sampling(data_small)\n",
    "# data_equal_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing Text Reviews\n",
    "def data_cleaning(df):\n",
    "    # Removing emtpy cells\n",
    "    df.dropna(inplace=True)\n",
    "    df['review_body_cleaned'] = df['review_body'].copy()\n",
    "    \n",
    "    # Removing Unicode Chars (URL)\n",
    "    df['review_body_cleaned'] = df['review_body_cleaned'].apply(\n",
    "        lambda rev: re.sub(r\"(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", rev))\n",
    "        \n",
    "    # Replace HTML keywords with blank space (\"&quot;\", \"br\", \"&#34\")\n",
    "    remove_dict = {\"<br /><br />\": \" \", \"<br />\": \" \", \"br \": \"\", \"&quot;\": \" \", \"&#34\": \" \",\n",
    "                   \"<BR>\": \" \", \"_\": \"\"}\n",
    "    for key, val in remove_dict.items():\n",
    "        df['review_body_cleaned'] = df['review_body_cleaned'].apply(\n",
    "            lambda x: x.replace(key, val))\n",
    "        \n",
    "    print(\"\\n######## Remove URL and HTML Keywords Complete ########\")\n",
    "    \n",
    "    # Remove Punctuations and numbers\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    df['review_body_cleaned'] = df['review_body_cleaned'].apply(\n",
    "        lambda x: ' '.join([word for word in tokenizer.tokenize(x)]))\n",
    "    \n",
    "    remove_dict = {\"0\": \"\", \"1\": \"\", \"2\": \"\", \"3\": \"\", \"4\": \"\", \"5\": \"\", \"6\": \"\", \"7\": \"\", \"8\": \"\", \"9\": \"\",\n",
    "                   \"(\": \"\", \")\":\"\"}\n",
    "    for key, val in remove_dict.items():\n",
    "        df['review_body_cleaned'] = df['review_body_cleaned'].apply(\n",
    "            lambda x: x.replace(key, val))\n",
    "    \n",
    "    print(\"\\n######## Remove Punctuation and Numbers Complete ########\")\n",
    "    \n",
    "    # Lowercase Words\n",
    "    df['review_body_cleaned'] = df['review_body_cleaned'].str.lower()\n",
    "    \n",
    "    print(\"\\n######## Lowercase Complete ########\")\n",
    "\n",
    "    # Remove Stop Words.\n",
    "    stop = stopwords.words('english')\n",
    "      \n",
    "    df['review_body_cleaned'] = df['review_body_cleaned'].apply(\n",
    "        lambda x: ' '.join([word for word in x.split() if word.strip() not in stop]))\n",
    "    \n",
    "    print(\"\\n######## Remove Stop Words Complete ########\")\n",
    "    \n",
    "    # Lemmatization using .lemma_\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "    df['review_body_cleaned'] = df['review_body_cleaned'].apply(\n",
    "        lambda x: ' '.join([token.lemma_ for token in nlp(x)]))\n",
    "    \n",
    "    print(\"\\n######## Data Cleaning Complete ########\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Clean the dataset\n",
    "data_cleaned = data_cleaning(data_equal_weight)\n",
    "data_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering + Prepare Data for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/48331315/how-to-extract-all-the-ngrams-from-a-text-dataframe-column-in-different-order-in\n",
    "\n",
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "from itertools import chain\n",
    "\n",
    "def find_ngrams(input_list, n):\n",
    "    return list(zip(*[input_list[i:] for i in range(n)]))\n",
    "\n",
    "def add_bigram_column(df):\n",
    "    copy = df.copy()\n",
    "    copy['bigrams'] = copy['review_body_cleaned'].map(lambda x: find_ngrams(x.split(), 2))\n",
    "    return copy\n",
    "    \n",
    "data_cleaned = add_bigram_column(data_cleaned)\n",
    "data_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's call the bigrams in vertified reviews \"gold_bigrams\" and the bigrams in unverified reviews \"fake_bigrams\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fake = data_cleaned[data_cleaned['verified_purchase'] == 'N']\n",
    "data_gold = data_cleaned[data_cleaned['verified_purchase'] == 'Y']\n",
    "\n",
    "data_fake.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gold_bigrams(df_cleaned):\n",
    "    global df_gold, gold_bigrams, gold_bigram_counts\n",
    "    df_gold = df_cleaned[df_cleaned['verified_purchase'] == 'Y']\n",
    "    gold_bigrams = df_gold['bigrams'].tolist()\n",
    "    gold_bigrams = list(chain(*gold_bigrams))\n",
    "    gold_bigram_counts = Counter(gold_bigrams)\n",
    "\n",
    "create_gold_bigrams(data_cleaned)\n",
    "gold_bigram_counts.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fake_bigrams(df_cleaned):\n",
    "    global df_fake, fake_bigrams, fake_bigram_counts\n",
    "    df_fake = df_cleaned[df_cleaned['verified_purchase'] == 'N']\n",
    "    fake_bigrams = df_fake['bigrams'].tolist()\n",
    "    fake_bigrams = list(chain(*fake_bigrams))\n",
    "    fake_bigram_counts = Counter(fake_bigrams)\n",
    "    \n",
    "create_fake_bigrams(data_cleaned)\n",
    "fake_bigram_counts.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_file_path = \"C:/Users/tsaie/OneDrive/Desktop/000 Resumes & Projects/# Projects/DS3 Fake Amazon Reviews/Bigrams/\"\n",
    "\n",
    "gold_bigram_counts_df = pd.DataFrame(gold_bigram_counts.most_common(), columns=['bigram','count'])\n",
    "gold_bigram_file_name = f'{category} gold_bigrams.csv'\n",
    "gold_bigram_counts_df.to_csv(bigram_file_path + gold_bigram_file_name)\n",
    "\n",
    "fake_bigram_counts_df = pd.DataFrame(fake_bigram_counts.most_common(), columns=['bigram','count'])\n",
    "fake_bigram_file_name = f'{category} fake_bigrams.csv'\n",
    "fake_bigram_counts_df.to_csv(bigram_file_path + fake_bigram_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_counter_N_most_common(gold_or_fake, N):\n",
    "#     counter = eval(f'{gold_or_fake}_bigram_counts')\n",
    "#     d = {}\n",
    "#     for tup in counter.most_common(N):\n",
    "#         bigram = f\"{tup[0][0]} {tup[0][1]}\"\n",
    "#         d[bigram] = tup[1]\n",
    "\n",
    "#     df = pd.Series(data=d).sort_values(ascending=False).to_frame().reset_index().rename(columns={'index': 'bigram', 0: 'count'})\n",
    "#     f, ax = plt.subplots(figsize=(8, N/4 + 1))\n",
    "#     if gold_or_fake == 'gold':\n",
    "#         palette = sns.color_palette('flare', n_colors=N)\n",
    "#     else:\n",
    "#         palette = sns.color_palette('crest', n_colors=N)\n",
    "#     palette.reverse()\n",
    "#     title = f'Most Popular Bigrams in {gold_or_fake.capitalize()} Reviews for the {category} Category'\n",
    "#     sns.barplot(x='count', y='bigram', data=df, orient='h', palette=palette).set_title(title)\n",
    "#     file_path = 'C:/Users/tsaie/OneDrive/Desktop/000 Resumes & Projects/# Projects/FARS/Ester Tsai (Bigram and ML)/Images/'\n",
    "#     plt.savefig(file_path + f'{title} (transparent).png', transparent=True)\n",
    "#     plt.savefig(file_path + f'{title}.png')\n",
    "    \n",
    "# plot_counter_N_most_common(\"fake\", 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from wordcloud import WordCloud\n",
    "\n",
    "# def wordcloud_N_most_common(gold_or_fake, N):\n",
    "#     counter = eval(f'{gold_or_fake}_bigram_counts')\n",
    "#     d = {}\n",
    "#     for tup in counter.most_common(N):\n",
    "#         bigram = f\"{tup[0][0]} {tup[0][1]}\"\n",
    "#         d[bigram] = tup[1]\n",
    "        \n",
    "# #     mask = np.array(Image.open(\"../input/input-img/cloud.png\"))\n",
    "\n",
    "#     word_cloud = WordCloud(width=3000, height=1500, background_color='white') #, mask=mask\n",
    "#     word_cloud.generate_from_frequencies(frequencies=d)\n",
    "\n",
    "#     plt.figure(figsize=(14,7))\n",
    "#     plt.tight_layout(pad=0)\n",
    "#     plt.imshow(word_cloud, interpolation='bilinear')\n",
    "#     plt.axis(\"off\")\n",
    "#     title = f'WORDCLOUD - Most Popular Bigrams in {gold_or_fake.capitalize()} Reviews for the {category} Category'\n",
    "#     plt.title(title, fontsize=15)\n",
    "\n",
    "#     file_path = 'C:/Users/tsaie/OneDrive/Desktop/000 Resumes & Projects/# Projects/FARS/Ester Tsai (Bigram and ML)/Images/'\n",
    "#     plt.savefig(file_path + f'{title} (transparent).png', bbox_inches='tight', transparent=True)\n",
    "#     plt.savefig(file_path + f'{title}.png', bbox_inches='tight')\n",
    "    \n",
    "#     plt.show()\n",
    "    \n",
    "# wordcloud_N_most_common('fake', 140)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Features\n",
    "\n",
    "**count** = number of gold/fake_bigrams in a review\n",
    "\n",
    "**percent** = number of gold/fake_bigrams as a percentage of total number of bigrams in a review.\n",
    "\n",
    "**simple score** = sum of the gold/fake_bigrams' popularity scores (calculated using the bigram's count in the Counter)\n",
    "\n",
    "**normalized score** = simple score / total bigram count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# data_cleaned[[('anywhere', 'find') in bigrams for bigrams in data_cleaned['bigrams']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigram_count_percent(bigrams, bigram_dict):\n",
    "    if len(bigrams) == 0:\n",
    "        return 0\n",
    "    \n",
    "    count = 0\n",
    "    for bigram in bigrams:\n",
    "        if bigram in bigram_dict.keys():\n",
    "            count += 1\n",
    "    return count / len(bigrams)\n",
    "\n",
    "def get_bigram_normalized_score(bigrams, bigram_dict):\n",
    "    if len(bigrams) == 0:\n",
    "        return 0\n",
    "    \n",
    "    score = 0\n",
    "    for bigram in bigrams:\n",
    "        if bigram in bigram_dict.keys():\n",
    "            score += bigram_dict[bigram]\n",
    "    return score / len(bigrams)\n",
    "\n",
    "def get_bigram_unique_count_percent(bigrams, bigram_dict, the_other_bigram_dict, unique_threshold=0):\n",
    "    # Count the number of bigrams in a review that appear in bigram_dict but not the_other_bigram_dict. \n",
    "    # Can adjust unique_threshold so you can count also the bigrams that appear in ...\n",
    "    # ...the_other_bigram_dict fewer than unique_threshold times\n",
    "    \n",
    "    if len(bigrams) == 0:\n",
    "        return 0\n",
    "    \n",
    "    count = len(bigrams)\n",
    "    for bigram in bigrams:\n",
    "        if (bigram in the_other_bigram_dict.keys()):\n",
    "            if (the_other_bigram_dict[bigram] > unique_threshold):\n",
    "                count -= 1\n",
    "    return count / len(bigrams)\n",
    "\n",
    "# Add bigram_count\n",
    "data_cleaned['bigram_count'] = data_cleaned['bigrams'].apply(lambda x: len(x))\n",
    "\n",
    "# Add features based on gold or fake bigrams\n",
    "fake_bigram_dict = dict(fake_bigram_counts)\n",
    "fake_bigram_dict_filtered = dict((k, v) for k, v in fake_bigram_dict_all.items() if v >= 2)\n",
    "\n",
    "gold_bigram_dict = dict(gold_bigram_counts)\n",
    "gold_bigram_dict_filtered = dict((k, v) for k, v in gold_bigram_dict_all.items() if v >= 2)\n",
    "\n",
    "for gold_or_fake in ['gold', 'fake']:\n",
    "\n",
    "    exec(f\"data_cleaned['{gold_or_fake}_bigram_percent'] = data_cleaned['bigrams'].apply(\\\n",
    "        lambda x: get_bigram_count_percent(x, {gold_or_fake}_bigram_dict_filtered))\")\n",
    "\n",
    "    exec(f\"data_cleaned['{gold_or_fake}_bigram_unique_percent'] = data_cleaned['bigrams'].apply(\\\n",
    "        lambda x: get_bigram_unique_count_percent(x, {gold_or_fake}_bigram_dict, {gold_or_fake}_bigram_dict, 0))\")\n",
    "\n",
    "    exec(f\"data_cleaned['{gold_or_fake}_bigram_normalized_score'] = data_cleaned['bigrams'].apply(\\\n",
    "        lambda x: get_bigram_normalized_score(x, {gold_or_fake}_bigram_dict))\")\n",
    "\n",
    "data_cleaned.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned[data_cleaned['verified_purchase'] == 'Y'].head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Use Machine Learning to Make Predictions for Verified VS. Unverified\n",
    "LABELS: \n",
    "- 1 = verified review\n",
    "- 0 = unverified review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semi_supervised_learning(df, model, algorithm, threshold=0.8, iterations=40):\n",
    "    df = df.copy()\n",
    "    \n",
    "    df_unlabled = df[['bigram_count', 'fake_bigram_percent', 'fake_bigram_normalized_score', 'fake_bigram_unique_percent',\n",
    "             'gold_bigram_percent', 'gold_bigram_normalized_score', 'gold_bigram_unique_percent']]\n",
    "#     df_unlabled = df[['bigram_count', 'fake_bigram_count', 'fake_bigram_percent', 'fake_bigram_simple_score', 'fake_bigram_normalized_score',\n",
    "#              'gold_bigram_count', 'gold_bigram_percent', 'gold_bigram_simple_score', 'gold_bigram_normalized_score']]\n",
    "\n",
    "    df['verified_purchase'] = df['verified_purchase'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "    print(\"Training \" + algorithm + \" Model\")\n",
    "    labels = df['verified_purchase']\n",
    "    \n",
    "    train_data, test_data, train_label, test_label = train_test_split(df_unlabled, labels, test_size=0.25, random_state=42)\n",
    "\n",
    "    test_data_copy = test_data.copy()\n",
    "    test_label_copy = test_label.copy()\n",
    "    \n",
    "    all_labeled = False\n",
    "\n",
    "    current_iteration = 0\n",
    "\n",
    "    pbar = tqdm(total=iterations)\n",
    "\n",
    "    while not all_labeled and (current_iteration < iterations):\n",
    "        current_iteration += 1\n",
    "        model.fit(train_data, train_label)\n",
    "\n",
    "        probabilities = model.predict_proba(test_data)\n",
    "        pseudo_labels = model.predict(test_data)\n",
    "\n",
    "        indices = np.argwhere(probabilities > threshold)\n",
    "\n",
    "        for item in indices:\n",
    "            train_data.loc[test_data.index[item[0]]] = test_data.iloc[item[0]]\n",
    "            train_label.loc[test_data.index[item[0]]] = pseudo_labels[item[0]]\n",
    "        test_data.drop(test_data.index[indices[:, 0]], inplace=True)\n",
    "        test_label.drop(test_label.index[indices[:, 0]], inplace=True)\n",
    "\n",
    "        print(\"--\" * 20)\n",
    "\n",
    "        if len(test_data) == 0:\n",
    "            print(\"Exiting loop\")\n",
    "            all_labeled = True\n",
    "        pbar.update(1)\n",
    "        \n",
    "    pbar.close()\n",
    "    predicted_labels = model.predict(test_data_copy)\n",
    "\n",
    "    print(algorithm + ' Model Results')\n",
    "    print('--' * 20)\n",
    "    print('Accuracy Score : ' + str(accuracy_score(test_label_copy, predicted_labels)))\n",
    "    print('Precision Score : ' + str(precision_score(test_label_copy, predicted_labels, pos_label=1)))\n",
    "    print('Recall Score : ' + str(recall_score(test_label_copy, predicted_labels, pos_label=1)))\n",
    "    print('F1 Score : ' + str(f1_score(test_label_copy, predicted_labels, pos_label=1)))\n",
    "#     print('Confusion Matrix : \\n' + str(confusion_matrix(test_label_copy, predicted_labels)))\n",
    "    plot_confusion_matrix(test_label_copy, predicted_labels, classes=[1, 0],\n",
    "                          title=algorithm + ' Confusion Matrix').show()\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes, title=None, cmap=plt.cm.Blues):\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           xticklabels=classes,\n",
    "           yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start_time = time()\n",
    "# rf = RandomForestClassifier(random_state=42, criterion='entropy', max_depth=14, max_features='auto', n_estimators=500)\n",
    "rf = RandomForestClassifier(random_state=42, criterion='entropy', max_depth=14, max_features='auto', n_estimators=500)\n",
    "rf_model = semi_supervised_learning(data_cleaned, model=rf, threshold=0.7, iterations=3, algorithm='Random Forest')\n",
    "end_time = time()\n",
    "\n",
    "print(\"Time taken : \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned[(data_cleaned['verified_purchase'] == 'N') & (data_cleaned['fake_bigram_normalized_score'] < data_cleaned['gold_bigram_normalized_score'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "ML_model_file_path = \"C:/Users/tsaie/OneDrive/Desktop/000 Resumes & Projects/# Projects/DS3 Fake Amazon Reviews/ML Models/\"\n",
    "ML_model_file_name = f'05_05_2022 rf_model {(category)}.joblib'\n",
    "dump(rf_model, ML_model_file_path + ML_model_file_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_end_time = time()\n",
    "print(round(overall_end_time - overall_start_time), \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_model = load(ML_model_file_path + ML_model_file_name) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEXT STEP: Making Predictions for a User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = 'Electronics'\n",
    "ML_model_file_path = \"C:/Users/tsaie/OneDrive/Desktop/000 Resumes & Projects/# Projects/FARS/Ester Tsai (Bigram and ML)/ML Models/\"\n",
    "ML_model_file_name = f'05_05_2022 rf_model {(category)}.joblib'\n",
    "rf_model = load(ML_model_file_path + ML_model_file_name) \n",
    "\n",
    "\n",
    "bigram_file_path = \"C:/Users/tsaie/OneDrive/Desktop/000 Resumes & Projects/# Projects/FARS/Ester Tsai (Bigram and ML)/Bigrams/\"\n",
    "\n",
    "gold_bigram_file_name = f'{(category)} gold_bigrams.csv'\n",
    "gold_bigram_df = pd.read_csv(bigram_file_path + gold_bigram_file_name, index_col=0)\n",
    "display(gold_bigram_df.head(3))\n",
    "\n",
    "fake_bigram_file_name = f'{(category)} fake_bigrams.csv'\n",
    "fake_bigram_df = pd.read_csv(bigram_file_path + fake_bigram_file_name, index_col=0)\n",
    "display(fake_bigram_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try changing the review and see what the RandomForest model predicts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = \"These are the best headphones I've bought in the past few years. Totally recommend!\"\n",
    "\n",
    "user_input_processed_df = clean_review_and_add_features(review, data_cleaned)\n",
    "display(user_input_processed_df)\n",
    "\n",
    "def prepare_df_for_prediction(processed_df):\n",
    "    df = processed_df.copy()\n",
    "    df = df[['fake_bigram_count', 'fake_bigram_percent', 'fake_bigram_simple_score', 'fake_bigram_normalized_score',\n",
    "             'gold_bigram_count', 'gold_bigram_percent', 'gold_bigram_simple_score', 'gold_bigram_normalized_score']]\n",
    "    return df\n",
    "\n",
    "df_for_prediction = prepare_df_for_prediction(user_input_processed_df)\n",
    "prediction, probabilities = rf_model.predict(df_for_prediction), rf_model.predict_proba(df_for_prediction)[0]\n",
    "\n",
    "def interpret_prediction(review, pred, proba):\n",
    "    proba = [round(proba[0], 3) * 100, round(proba[1], 3) * 100]\n",
    "    if prediction[0] == 1:\n",
    "        print(f'\"{review}\" is predicted to be a VERIFIED review, with {proba[1]}% probability of being VERIFIED and {proba[0]}% probability of being UNVERIFIED')\n",
    "    if prediction[0] == 0:\n",
    "        print(f'\"{review}\" is predicted to be an UNVERIFIED review, with {proba[0]}% probability of being UNVERIFIED and {proba[1]}% probability of being VERIFIED')\n",
    "        \n",
    "interpret_prediction(review, prediction, probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# From Example Code  https://github.com/Swathiu/Detecting-Fake-Reviews/blob/master/Deception_Detection.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, pairwise_distances\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"C:/Users/tsaie/OneDrive/Desktop/000 Resumes & Projects/# Projects/DS3 Fake Amazon Reviews/Dataset/\"\n",
    "apparel = pd.read_csv(file_path + 'amazon_reviews_us_Apparel_v1_00.tsv.gz', compression='gzip', header=0, sep='\\t', quotechar='\"', error_bad_lines=False, warn_bad_lines=False, nrows=10_000)\n",
    "# electronics = pd.read_csv(file_path + 'amazon_reviews_us_Apparel_v1_00.tsv.gz', compression='gzip', header=0, sep='\\t', quotechar='\"', error_bad_lines=False, warn_bad_lines=False)\n",
    "apparel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apparel_small = apparel[['verified_purchase', 'review_body']]\n",
    "apparel_small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(df):\n",
    "    print(\"######## Cleaning Data ########\")\n",
    "\n",
    "    # Removing emtpy cells\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # Pre-processing Text Reviews\n",
    "    \n",
    "    # Lowercase Words\n",
    "    df['review_body'] = df['review_body'].apply(\n",
    "        lambda x: x.lower())\n",
    "    \n",
    "    print(\"\\n######## Lowercase Complete ########\")\n",
    "\n",
    "    # Remove Stop Words. Also remove \"br \" (HTML line break symbols) and \"&#34\" (HTML quote symbols)\n",
    "    stop = stopwords.words('english')\n",
    "    stop += [\"br\", \"&#34\"]\n",
    "                \n",
    "    df['review_body'] = df['review_body'].apply(\n",
    "        lambda x: ' '.join([word for word in x.split() if word.strip() not in stop]))\n",
    "    \n",
    "    df['review_body'] = df['review_body'].apply(\n",
    "        lambda x: x.replace(\"<br /><br />\", \" \"))\n",
    "    \n",
    "    print(\"\\n######## Remove Stop Words Complete ########\")\n",
    "\n",
    "    # Remove Punctuations\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    df['review_body'] = df['review_body'].apply(\n",
    "        lambda x: ' '.join([word for word in tokenizer.tokenize(x)]))\n",
    "    \n",
    "    print(\"\\n######## Remove Punctuation Complete ########\")\n",
    "    \n",
    "    # Lemmatization using .lemma_\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "    df['review_body'] = df['review_body'].apply(\n",
    "        lambda x: ' '.join([token.lemma_ for token in nlp(x)]))\n",
    "    \n",
    "    print(\"\\n######## Data Cleaning Complete ########\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "apparel_cleaned = data_cleaning(apparel_small)\n",
    "apparel_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/48331315/how-to-extract-all-the-ngrams-from-a-text-dataframe-column-in-different-order-in\n",
    "\n",
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "from itertools import chain\n",
    "\n",
    "def find_ngrams(input_list, n):\n",
    "    return list(zip(*[input_list[i:] for i in range(n)]))\n",
    "\n",
    "\n",
    "apparel_cleaned['bigrams'] = apparel_cleaned['review_body'].map(lambda x: find_ngrams(x.split(\" \"), 2))\n",
    "apparel_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apparel_un_verified = apparel_cleaned[apparel_cleaned['verified_purchase'] == 'N']\n",
    "apparel_verified = apparel_cleaned[apparel_cleaned['verified_purchase'] == 'Y']\n",
    "\n",
    "apparel_un_verified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verified_bigrams = apparel_verified['bigrams'].tolist()\n",
    "verified_bigrams = list(chain(*verified_bigrams))\n",
    "\n",
    "verified_bigram_counts = Counter(verified_bigrams)\n",
    "verified_bigram_counts.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "un_verified_bigrams = apparel_un_verified['bigrams'].tolist()\n",
    "un_verified_bigrams = list(chain(*un_verified_bigrams))\n",
    "\n",
    "un_verified_bigram_counts = Counter(un_verified_bigrams)\n",
    "un_verified_bigram_counts.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering + Prepare Data for Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def under_sampling(df):\n",
    "    print(\"Under-Sampling Data\")\n",
    "    # Count of Reviews\n",
    "    print(\"Verified:\", len(df[(df['verified_purchase'] == 'Y')]))\n",
    "    print(\"Un-Verified:\", len(df[(df['verified_purchase'] == 'N')]))\n",
    "\n",
    "    sample_size = len(df[(df['verified_purchase'] == 'N')])\n",
    "\n",
    "    authentic_reviews_df = df[df['verified_purchase'] == 'Y']\n",
    "    fake_reviews_df = df[df['verified_purchase'] == 'N']\n",
    "\n",
    "    authentic_reviews_us_df = authentic_reviews_df.sample(sample_size)\n",
    "    under_sampled_df = pd.concat([authentic_reviews_us_df, fake_reviews_df], axis=0)\n",
    "\n",
    "    print(\"Under-Sampled Verified\", len(under_sampled_df[(under_sampled_df['verified_purchase'] == 'Y')]))\n",
    "    print(\"Under-Sampled Un-Verified\", len(under_sampled_df[(under_sampled_df['verified_purchase'] == 'N')]))\n",
    "    \n",
    "\n",
    "    # Graph of Data Distribution\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    sns.countplot(x='verified_purchase', data=under_sampled_df)\n",
    "    plt.title(\"Count of Reviews\")\n",
    "    plt.show()\n",
    "    print(\"Under-Sampling Complete\")\n",
    "    return under_sampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apparel_equal_weight = under_sampling(apparel_cleaned)\n",
    "apparel_equal_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's call the bigrams in vertified reviews \"gold_bigrams\" and the bigrams in unverified reviews \"fake_bigrams\"\n",
    "\n",
    "**count** = number of gold/fake_bigrams in a review\n",
    "\n",
    "**percent** = number of gold/fake_bigrams as a percentage of total number of bigrams in a review.\n",
    "\n",
    "**simple score** = sum of the gold/fake_bigrams' popularity scores (calculated using simply the bigram's count in the Counter)\n",
    "\n",
    "**normalized score** = simple score / total bigram count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigram_count(bigrams, bigram_dict):\n",
    "    count = 0\n",
    "    for bigram in bigrams:\n",
    "        if bigram in bigram_dict.keys():\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def get_bigram_simple_score(bigrams, bigram_dict):\n",
    "    score = 0\n",
    "    for bigram in bigrams:\n",
    "        if bigram in bigram_dict.keys():\n",
    "            score += bigram_dict[bigram]\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apparel_equal_weight['bigram_count'] = apparel_equal_weight['bigrams'].apply(\n",
    "    lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake\n",
    "\n",
    "fake_bigram_dict = dict(un_verified_bigram_counts) # fake_bigram_dict = dict(un_verified_bigram_counts.most_common(30))\n",
    "\n",
    "apparel_equal_weight['fake_bigram_count'] = apparel_equal_weight['bigrams'].apply(\n",
    "    lambda x: get_bigram_count(x, fake_bigram_dict))\n",
    "\n",
    "apparel_equal_weight['fake_bigram_percent'] = apparel_equal_weight['fake_bigram_count'] / apparel_equal_weight['bigram_count']\n",
    "\n",
    "apparel_equal_weight['fake_bigram_simple_score'] = apparel_equal_weight['bigrams'].apply(\n",
    "    lambda x: get_bigram_simple_score(x, fake_bigram_dict))\n",
    "\n",
    "apparel_equal_weight['fake_bigram_normalized_score'] = apparel_equal_weight['fake_bigram_simple_score'] / apparel_equal_weight['bigram_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gold\n",
    "\n",
    "gold_bigram_dict = dict(verified_bigram_counts) # gold_bigram_dict = dict(verified_bigram_counts.most_common(30))\n",
    "\n",
    "apparel_equal_weight['gold_bigram_count'] = apparel_equal_weight['bigrams'].apply(\n",
    "    lambda x: get_bigram_count(x, gold_bigram_dict))\n",
    "\n",
    "apparel_equal_weight['gold_bigram_percent'] = apparel_equal_weight['gold_bigram_count'] / apparel_equal_weight['bigram_count']\n",
    "\n",
    "apparel_equal_weight['gold_bigram_simple_score'] = apparel_equal_weight['bigrams'].apply(\n",
    "    lambda x: get_bigram_simple_score(x, gold_bigram_dict))\n",
    "\n",
    "apparel_equal_weight['gold_bigram_normalized_score'] = apparel_equal_weight['gold_bigram_simple_score'] / apparel_equal_weight['bigram_count']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apparel_equal_weight = apparel_equal_weight.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apparel_equal_weight[apparel_equal_weight['verified_purchase'] == 'N']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Use Machine Learning to Make Predictions for Verified VS. Unverified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semi_supervised_learning(df, model, algorithm, threshold=0.8, iterations=40):\n",
    "    df = df.copy()\n",
    "    \n",
    "    df_unlabled = df[['fake_bigram_count', 'fake_bigram_percent', 'fake_bigram_simple_score', 'fake_bigram_normalized_score',\n",
    "             'gold_bigram_count', 'gold_bigram_percent', 'gold_bigram_simple_score', 'gold_bigram_normalized_score']]\n",
    "\n",
    "    df['verified_purchase'] = df['verified_purchase'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "    print(\"Training \" + algorithm + \" Model\")\n",
    "    labels = df['verified_purchase']\n",
    "    \n",
    "    train_data, test_data, train_label, test_label = train_test_split(df_unlabled, labels, test_size=0.25, random_state=42)\n",
    "\n",
    "    test_data_copy = test_data.copy()\n",
    "    test_label_copy = test_label.copy()\n",
    "    \n",
    "    all_labeled = False\n",
    "\n",
    "    current_iteration = 0\n",
    "\n",
    "    pbar = tqdm(total=iterations)\n",
    "\n",
    "    while not all_labeled and (current_iteration < iterations):\n",
    "        current_iteration += 1\n",
    "        model.fit(train_data, train_label)\n",
    "\n",
    "        probabilities = model.predict_proba(test_data)\n",
    "        pseudo_labels = model.predict(test_data)\n",
    "\n",
    "        indices = np.argwhere(probabilities > threshold)\n",
    "\n",
    "        for item in indices:\n",
    "            train_data.loc[test_data.index[item[0]]] = test_data.iloc[item[0]]\n",
    "            train_label.loc[test_data.index[item[0]]] = pseudo_labels[item[0]]\n",
    "        test_data.drop(test_data.index[indices[:, 0]], inplace=True)\n",
    "        test_label.drop(test_label.index[indices[:, 0]], inplace=True)\n",
    "\n",
    "        print(\"--\" * 20)\n",
    "\n",
    "        if len(test_data) == 0:\n",
    "            print(\"Exiting loop\")\n",
    "            all_labeled = True\n",
    "        pbar.update(1)\n",
    "        \n",
    "    pbar.close()\n",
    "    predicted_labels = model.predict(test_data_copy)\n",
    "\n",
    "    print(algorithm + ' Model Results')\n",
    "    print('--' * 20)\n",
    "    print('Accuracy Score : ' + str(accuracy_score(test_label_copy, predicted_labels)))\n",
    "    print('Precision Score : ' + str(precision_score(test_label_copy, predicted_labels, pos_label=1)))\n",
    "    print('Recall Score : ' + str(recall_score(test_label_copy, predicted_labels, pos_label=1)))\n",
    "    print('F1 Score : ' + str(f1_score(test_label_copy, predicted_labels, pos_label=1)))\n",
    "    print('Confusion Matrix : \\n' + str(confusion_matrix(test_label_copy, predicted_labels)))\n",
    "    plot_confusion_matrix(test_label_copy, predicted_labels, classes=[1, 0],\n",
    "                          title=algorithm + ' Confusion Matrix').show()\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes, title=None, cmap=plt.cm.Blues):\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           xticklabels=classes,\n",
    "           yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "    return plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time()\n",
    "rf = RandomForestClassifier(random_state=42, criterion='entropy', max_depth=14, max_features='auto', n_estimators=500)\n",
    "semi_supervised_learning(apparel_equal_weight, model=rf, threshold=0.7, iterations=15, algorithm='Random Forest')\n",
    "end_time = time()\n",
    "\n",
    "print(\"Time taken : \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time()\n",
    "nb = GaussianNB()\n",
    "semi_supervised_learning(apparel_equal_weight, model=nb, threshold=0.7, iterations=15, algorithm='Naive Bayes')\n",
    "end_time = time()\n",
    "\n",
    "print(\"Time taken : \", end_time - start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

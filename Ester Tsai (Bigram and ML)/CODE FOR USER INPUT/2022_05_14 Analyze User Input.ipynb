{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# streamlit directory\n",
    "# cd \"C:/Users/tsaie/OneDrive/Desktop/000 Resumes & Projects/# Projects/FARS/python app - streamlit\"\n",
    "# streamlit run app.py\n",
    "\n",
    "from joblib import dump, load\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, pairwise_distances\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "import spacy\n",
    "\n",
    "pd.set_option('display.max_columns', None) \n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE category\n",
    "category = 'Electronics'\n",
    "\n",
    "# CHANGE general_file_path\n",
    "general_file_path = \"C:/Users/tsaie/OneDrive/Desktop/000 Resumes & Projects/# Projects/FARS/\"\n",
    "\n",
    "\n",
    "\n",
    "# Load the rf_model, gold_bigram_df, and fake_bigram_df\n",
    "ML_model_file_path = general_file_path + \"Ester Tsai (Bigram and ML)/ML Models/\"\n",
    "\n",
    "ML_model_file_name = f'2022_05_11 rf_model {(category)}.joblib'\n",
    "rf_model = load(ML_model_file_path + ML_model_file_name) \n",
    "\n",
    "bigram_file_path = general_file_path + \"Ester Tsai (Bigram and ML)/Bigrams/\"\n",
    "\n",
    "gold_bigram_file_name = f'{(category)} gold_bigrams.csv'\n",
    "gold_bigram_df = pd.read_csv(bigram_file_path + gold_bigram_file_name, index_col=0)\n",
    "\n",
    "fake_bigram_file_name = f'{(category)} fake_bigrams.csv'\n",
    "fake_bigram_df = pd.read_csv(bigram_file_path + fake_bigram_file_name, index_col=0)\n",
    "\n",
    "\n",
    "def data_cleaning(df):\n",
    "    \n",
    "    start_1 = time()\n",
    "    \n",
    "    # Removing emtpy cells\n",
    "    df.dropna(inplace=True)\n",
    "    df['review_cleaned'] = df['review_body'].copy()\n",
    "    \n",
    "    # Removing Unicode Chars (URL)\n",
    "    df['review_cleaned'] = df['review_cleaned'].apply(\n",
    "        lambda rev: re.sub(r\"(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", rev))\n",
    "        \n",
    "    # Replace HTML keywords with blank space (\"&quot;\", \"br\", \"&#34\")\n",
    "    remove_dict = {\"<br /><br />\": \" \", \"<br />\": \" \", \"br \": \"\", \"&quot;\": \" \", \"&#34\": \" \",\n",
    "                   \"<BR>\": \" \", \"_\": \"\"}\n",
    "    for key, val in remove_dict.items():\n",
    "        df['review_cleaned'] = df['review_cleaned'].apply(\n",
    "            lambda x: x.replace(key, val))\n",
    "        \n",
    "    end_1 = time()\n",
    "        \n",
    "    print(f\"\\n######## [{end_1 - start_1:0.2f} secs] Remove URL and HTML Keywords Complete ########\")\n",
    "    \n",
    "    start_2 = time()\n",
    "    \n",
    "    # Remove Punctuations and numbers\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    df['review_cleaned'] = df['review_cleaned'].apply(\n",
    "        lambda x: ' '.join([word for word in tokenizer.tokenize(x)]))\n",
    "    \n",
    "    remove_dict = {\"0\": \"\", \"1\": \"\", \"2\": \"\", \"3\": \"\", \"4\": \"\", \"5\": \"\", \"6\": \"\", \"7\": \"\", \"8\": \"\", \"9\": \"\",\n",
    "                   \"(\": \"\", \")\":\"\"}\n",
    "    for key, val in remove_dict.items():\n",
    "        df['review_cleaned'] = df['review_cleaned'].apply(\n",
    "            lambda x: x.replace(key, val))\n",
    "    \n",
    "    end_2 = time()\n",
    "    \n",
    "    print(f\"\\n######## [{end_2 - start_2:0.2f} secs] Remove Punctuation and Numbers Complete ########\")\n",
    "    \n",
    "    start_3 = time()\n",
    "    \n",
    "    # Lowercase Words\n",
    "    df['review_cleaned'] = df['review_cleaned'].str.lower()\n",
    "    \n",
    "    end_3 = time()\n",
    "    \n",
    "    print(f\"\\n######## [{end_3 - start_3:0.2f} secs] Lowercase Complete ########\")\n",
    "    \n",
    "    start_4 = time()\n",
    "\n",
    "    # Remove Stop Words.\n",
    "    stop = stopwords.words('english')\n",
    "      \n",
    "    df['review_cleaned'] = df['review_cleaned'].apply(\n",
    "        lambda x: ' '.join([word for word in x.split() if word.strip() not in stop]))\n",
    "    \n",
    "    end_4 = time()\n",
    "    \n",
    "    print(f\"\\n######## [{end_4 - start_4:0.2f} secs] Remove Stop Words Complete ########\")\n",
    "    \n",
    "    start_5 = time()\n",
    "    \n",
    "    # Lemmatization using .lemma_\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "    df['review_cleaned'] = df['review_cleaned'].apply(\n",
    "        lambda x: ' '.join([token.lemma_ for token in nlp(x)]))\n",
    "    \n",
    "    end_5 = time()\n",
    "    \n",
    "    print(f\"\\n######## [{end_5 - start_5:0.2f} secs] Lemmatization Complete ########\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "from itertools import chain\n",
    "\n",
    "def find_ngrams(input_list, n):\n",
    "    return list(zip(*[input_list[i:] for i in range(n)]))\n",
    "\n",
    "def add_bigram_column(df):\n",
    "    copy = df.copy()\n",
    "    copy['bigrams'] = copy['review_cleaned'].map(lambda x: find_ngrams(x.split(), 2))\n",
    "    return copy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_bigram_count(bigrams, bigram_dict):\n",
    "    if len(bigrams) == 0:\n",
    "        return 0\n",
    "    \n",
    "    count = 0\n",
    "    for bigram in bigrams:\n",
    "        if bigram in bigram_dict.keys():\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def get_count_percent(bigrams, bigram_dict):\n",
    "    if len(bigrams) == 0:\n",
    "        return 0\n",
    "    \n",
    "    count = get_bigram_count(bigrams, bigram_dict)\n",
    "    return count / len(bigrams)\n",
    "\n",
    "def get_averge_top_score(bigrams, bigram_dict, topN=5):\n",
    "    if len(bigrams) == 0:\n",
    "        return 0\n",
    "    \n",
    "    scores = np.array([])\n",
    "    for bigram in bigrams:\n",
    "        if bigram in bigram_dict.keys():\n",
    "            scores = np.append(scores, bigram_dict[bigram])\n",
    "    \n",
    "    if len(bigrams) <= topN:\n",
    "        return scores.mean()\n",
    "    \n",
    "    sort_descending = -np.sort(-scores)[:topN]\n",
    "    avg_top_score = sort_descending.mean()\n",
    "    return avg_top_score\n",
    "\n",
    "def get_normalized_score(bigrams, bigram_dict):\n",
    "    if len(bigrams) == 0:\n",
    "        return 0\n",
    "    \n",
    "    score = 0\n",
    "    for bigram in bigrams:\n",
    "        if bigram in bigram_dict.keys():\n",
    "            score += bigram_dict[bigram]\n",
    "    return score / len(bigrams)\n",
    "\n",
    "def get_unique_percent(bigrams, bigram_dict, the_other_bigram_dict, unique_threshold=0):\n",
    "    # Count the number of bigrams in a review that appear in bigram_dict but not the_other_bigram_dict. \n",
    "    # Can adjust unique_threshold so you can count also the bigrams that appear in ...\n",
    "    # ...the_other_bigram_dict fewer than unique_threshold times\n",
    "    \n",
    "    if len(bigrams) == 0:\n",
    "        return 0\n",
    "    \n",
    "    count = get_bigram_count(bigrams, bigram_dict)\n",
    "    for bigram in bigrams:\n",
    "        if bigram in the_other_bigram_dict.keys():\n",
    "            if the_other_bigram_dict[bigram] > unique_threshold:\n",
    "                count -= 1\n",
    "    return count / len(bigrams)\n",
    "\n",
    "def has_fake_keywords(bigrams):\n",
    "    fake_keywords = ['honest', 'unbiased', 'unbias', 'biased', 'bias', 'neutral', 'impartial', 'truthful', \n",
    "                     'discount', 'free', 'promotion', 'promote', 'complimentary', 'test', 'influence', 'influencer',\n",
    "                     'independent']\n",
    "    fake_tuples = [('receive', 'product'), ('product', 'receive'), ('provide', 'review'), \n",
    "                   ('product', 'exchange'), ('exchange', 'review'), ('review', 'opinion'),\n",
    "                   ('sample', 'provide'), ('provide', 'sample'), ('sample', 'review'), ('review', 'sample'), \n",
    "                   ('sample', 'product'), ('supply', 'sample'), ('receive', 'sample'), ('sample', 'receive')] \n",
    "    \n",
    "    for kw in fake_tuples:\n",
    "        if kw in bigrams:\n",
    "            return True\n",
    "            \n",
    "    for bigram in bigrams:\n",
    "        for kw in fake_keywords:\n",
    "            if kw in bigram:\n",
    "                return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def add_gold_fake_features(df):\n",
    "    \n",
    "    for gold_or_fake in ['gold', 'fake']:\n",
    "\n",
    "        exec(f\"df['{gold_or_fake}%'] = df['bigrams'].apply(\\\n",
    "            lambda x: get_count_percent(x, {gold_or_fake}_bigram_dict_filtered))\")\n",
    "        \n",
    "        exec(f\"df['{gold_or_fake}_unique%'] = df['bigrams'].apply(\\\n",
    "            lambda x: get_unique_percent(x, {gold_or_fake}_bigram_dict, {gold_or_fake}_bigram_dict, 3))\")\n",
    "\n",
    "        exec(f\"df['{gold_or_fake}_score'] = df['bigrams'].apply(\\\n",
    "            lambda x: get_normalized_score(x, {gold_or_fake}_bigram_dict))\")\n",
    "        \n",
    "        exec(f\"df['{gold_or_fake}_top_score'] = df['bigrams'].apply(\\\n",
    "            lambda x: get_averge_top_score(x, {gold_or_fake}_bigram_dict, 1))\")\n",
    "        \n",
    "        exec(f\"df['{gold_or_fake}_top_avg_score'] = df['bigrams'].apply(\\\n",
    "            lambda x: get_averge_top_score(x, {gold_or_fake}_bigram_dict, 5))\")\n",
    "\n",
    "    df['has_fake_keywords'] = df['bigrams'].apply(lambda x: has_fake_keywords(x))\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def gold_bigram_df_to_vars(gold_bigram_df):\n",
    "    global gold_bigrams, gold_bigram_dict, gold_bigram_dict_filtered\n",
    "    gold_bigrams = gold_bigram_df['bigram'].to_list()\n",
    "    str_to_tuple = lambda x: (x.split(\"'\")[1], x.split(\"'\")[3])\n",
    "    gold_bigrams = list(map(str_to_tuple, gold_bigrams))\n",
    "    gold_bigram_dict = {gold_bigrams[i]: gold_bigram_df['count'].iloc[i] for i in range(len(gold_bigrams))}\n",
    "    gold_bigram_dict_filtered = dict((k, v) for k, v in gold_bigram_dict.items() if v >= 2)\n",
    "    \n",
    "def fake_bigram_df_to_vars(fake_bigram_df):\n",
    "    global fake_bigrams, fake_bigram_dict, fake_bigram_dict_filtered\n",
    "    fake_bigrams = fake_bigram_df['bigram'].to_list()\n",
    "    str_to_tuple = lambda x: (x.split(\"'\")[1], x.split(\"'\")[3])\n",
    "    fake_bigrams = list(map(str_to_tuple, fake_bigrams))\n",
    "    fake_bigram_dict = {fake_bigrams[i]: fake_bigram_df['count'].iloc[i] for i in range(len(fake_bigrams))}\n",
    "    fake_bigram_dict_filtered = dict((k, v) for k, v in fake_bigram_dict.items() if v >= 2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def clean_review_and_add_features(review, gold_bigram_df, fake_bigram_df):\n",
    "    \n",
    "    # Create dataframe for the input review\n",
    "    df = pd.DataFrame(data={'review_body': review}, index=[0])\n",
    "    \n",
    "    # Create helper variables\n",
    "#     gold_bigram_df_to_vars(gold_bigram_df)\n",
    "#     fake_bigram_df_to_vars(fake_bigram_df)\n",
    "    \n",
    "    # Clean the user input\n",
    "    df = data_cleaning(df) \n",
    "    \n",
    "    # Add the column 'bigrams'\n",
    "    df['bigrams'] = df['review_cleaned'].map(lambda x: find_ngrams(x.split(), 2))\n",
    "    df['bigram_count'] = df['bigrams'].apply(lambda x: len(x))\n",
    "    \n",
    "    # Add features to score the user input\n",
    "    df = add_gold_fake_features(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create helper variables (TAKES ~20 SECONDS TO LOAD)\n",
    "gold_bigram_df_to_vars(gold_bigram_df)\n",
    "fake_bigram_df_to_vars(fake_bigram_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try changing the review and see what the RandomForest model predicts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######## [0.00 secs] Remove URL and HTML Keywords Complete ########\n",
      "\n",
      "######## [0.00 secs] Remove Punctuation and Numbers Complete ########\n",
      "\n",
      "######## [0.00 secs] Lowercase Complete ########\n",
      "\n",
      "######## [0.00 secs] Remove Stop Words Complete ########\n",
      "\n",
      "######## [0.38 secs] Lemmatization Complete ########\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_cleaned</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>bigram_count</th>\n",
       "      <th>gold%</th>\n",
       "      <th>gold_unique%</th>\n",
       "      <th>gold_score</th>\n",
       "      <th>gold_top_score</th>\n",
       "      <th>gold_top_avg_score</th>\n",
       "      <th>fake%</th>\n",
       "      <th>fake_unique%</th>\n",
       "      <th>fake_score</th>\n",
       "      <th>fake_top_score</th>\n",
       "      <th>fake_top_avg_score</th>\n",
       "      <th>has_fake_keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Electronics Category: I received product in exchange for an honest review of this headphone</td>\n",
       "      <td>electronic category receive product exchange honest review headphone</td>\n",
       "      <td>[(electronic, category), (category, receive), (receive, product), (product, exchange), (exchange, honest), (honest, review), (review, headphone)]</td>\n",
       "      <td>7</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>13.857143</td>\n",
       "      <td>64.0</td>\n",
       "      <td>24.25</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>369.142857</td>\n",
       "      <td>857.0</td>\n",
       "      <td>516.8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                   review_body  \\\n",
       "0  Electronics Category: I received product in exchange for an honest review of this headphone   \n",
       "\n",
       "                                                         review_cleaned  \\\n",
       "0  electronic category receive product exchange honest review headphone   \n",
       "\n",
       "                                                                                                                                             bigrams  \\\n",
       "0  [(electronic, category), (category, receive), (receive, product), (product, exchange), (exchange, honest), (honest, review), (review, headphone)]   \n",
       "\n",
       "   bigram_count     gold%  gold_unique%  gold_score  gold_top_score  \\\n",
       "0             7  0.428571      0.142857   13.857143            64.0   \n",
       "\n",
       "   gold_top_avg_score     fake%  fake_unique%  fake_score  fake_top_score  \\\n",
       "0               24.25  0.714286           0.0  369.142857           857.0   \n",
       "\n",
       "   fake_top_avg_score  has_fake_keywords  \n",
       "0               516.8               True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "REVIEW: \"Electronics Category: I received product in exchange for an honest review of this headphone\" \n",
      "\n",
      "PREDICTION: UNVERIFIED (81.0%) | VERIFIED (19.0%)\n"
     ]
    }
   ],
   "source": [
    "review = \"Electronics Category: I received product in exchange for an honest review of this headphone\"\n",
    "\n",
    "# Clean the user input and create a dataframe for it\n",
    "user_input_processed_df = clean_review_and_add_features(review, gold_bigram_df, fake_bigram_df)\n",
    "\n",
    "\n",
    "\n",
    "# DISPLAY THIS TABLE!!!!!!!!!!!!!@*^#@*Q^&#*&^@(#^(@*&#(@*&(&!(*&@*&(*^@($&^(@&^(&^(769798327498379)))))))))) LOLOLOL\n",
    "display(user_input_processed_df)\n",
    "\n",
    "\n",
    "\n",
    "def prepare_df_for_prediction(processed_df):\n",
    "    features = ['bigram_count', \n",
    "            'fake%', 'fake_unique%', 'fake_score', 'fake_top_avg_score', 'fake_top_score',\n",
    "            'gold%', 'gold_unique%', 'gold_score', 'gold_top_avg_score', 'gold_top_score',\n",
    "            'has_fake_keywords']\n",
    "    return processed_df[features]\n",
    "\n",
    "df_for_prediction = prepare_df_for_prediction(user_input_processed_df)\n",
    "\n",
    "prediction, probabilities = rf_model.predict(df_for_prediction)[0], rf_model.predict_proba(df_for_prediction)[0]\n",
    "# prediction: 0 = unverified, 1 = verified\n",
    "# probabilities[0] = prob of unverified  |  probabilities[1] = prob of verified\n",
    "\n",
    "\n",
    "def interpret_prediction(review, pred, proba):\n",
    "    prob_unverified, prob_verified = round(proba[0] * 100, 1), round(proba[1] * 100, 1)\n",
    "    print(f'\\nREVIEW: \"{review}\" \\n')\n",
    "    if pred == 1:\n",
    "        print(f'PREDICTION: VERIFIED ({prob_verified}%) | UNVERIFIED ({prob_unverified}%)')\n",
    "    if pred == 0:\n",
    "        print(f'PREDICTION: UNVERIFIED ({prob_unverified}%) | VERIFIED ({prob_verified}%)')\n",
    "\n",
    "interpret_prediction(review, prediction, probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What the Features Mean (PLEASE PUT THIS ON STREAMLIT)\n",
    "\n",
    "To reduce word space, let \"gold\" represent \"verified,\" and let \"fake\" represent \"unverified.\"\n",
    "NOTE: An \"unverified\" review isn't necessarily fake. A review is defined as \"unverified\" if the user did not buy the product from Amazon.\n",
    "\n",
    "### Setup\n",
    "- <code>gold_bigram_dict</code> contains all the bigrams that appeared in verified reviews and their # of occurrences \n",
    "- <code>fake_bigram_dict</code> contains all the bigrams that appeared in unverified reviews and their # of occurrences \n",
    "\n",
    "### Features\n",
    "<code>bigram_count</code> = # of bigrams in the review\n",
    "\n",
    "<code>gold%</code> = # bigrams in the review that appear in <code>gold_bigram_dict</code> at least 2 times / bigram_count\n",
    "\n",
    "<code>gold_unique%</code> = (# of bigrams that exist in <code>gold_bigram_dict</code> - # of bigrams that appear in <code>fake_bigram_dict</code> at least 3 times) / bigram_count\n",
    "\n",
    "<code>gold_score</code> = sum of all the bigrams' # of occurrences in verified reviews / bigram_count\n",
    "\n",
    "<code>gold_top_avg_score</code> = sum of the top 5 bigrams' # of occurrences in verified reviews / 5\n",
    "\n",
    "<code>gold_top_score</code> = highest # of occurrences in verified reviews\n",
    "\n",
    "Similar logic for the features <code>fake%</code>, <code>fake_unique%</code>, <code>fake_score</code>, <code>fake_top_avg_score</code>, and <code>fake_top_score</code>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gold & Fake Reviews from the Datasets (IGNORE FOR NOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Electronics\n",
    "\n",
    "### Gold:\n",
    "\n",
    "\n",
    "### Fake:\n",
    "\n",
    "Turbo charger, small and portable and fits perfectly in your pocket! A nice boost over regular chargers. I received this in exchange for an unbiased review.\n",
    "\n",
    "This is a thick high quality cable, my picture is crystal clear and this is just the right length for my needs, I received this in exchange for my honest review.\n",
    "\n",
    "Small, lightweight, fits on my desk. Does not come with batteries, I already had some. I received a discount for this review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beauty\n",
    "\n",
    "### Gold:\n",
    "\n",
    "\n",
    "### Fake:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toys\n",
    "\n",
    "### Gold:\n",
    "\n",
    "\n",
    "### Fake:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Office Products\n",
    "\n",
    "### Gold:\n",
    "\n",
    "\n",
    "### Fake:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apparel\n",
    "\n",
    "### Gold:\n",
    "\n",
    "\n",
    "### Fake:\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# streamlit directory\n",
    "# cd \"C:/Users/tsaie/OneDrive/Desktop/000 Resumes & Projects/# Projects/FARS/python app - streamlit\"\n",
    "# streamlit run app.py\n",
    "\n",
    "from joblib import dump, load\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, pairwise_distances\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "import spacy\n",
    "\n",
    "pd.set_option('display.max_columns', None) \n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigram</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>('great', 'product')</td>\n",
       "      <td>4201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>('work', 'well')</td>\n",
       "      <td>3831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>('work', 'great')</td>\n",
       "      <td>3506</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 bigram  count\n",
       "0  ('great', 'product')   4201\n",
       "1      ('work', 'well')   3831\n",
       "2     ('work', 'great')   3506"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigram</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>('use', 'product')</td>\n",
       "      <td>7186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>('skin', 'feel')</td>\n",
       "      <td>5759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>('work', 'well')</td>\n",
       "      <td>5663</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               bigram  count\n",
       "0  ('use', 'product')   7186\n",
       "1    ('skin', 'feel')   5759\n",
       "2    ('work', 'well')   5663"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CHANGE category\n",
    "category = 'Beauty'\n",
    "\n",
    "# CHANGE ML_model_file_path\n",
    "ML_model_file_path = \"C:/Users/tsaie/OneDrive/Desktop/000 Resumes & Projects/# Projects/FARS/Ester Tsai (Bigram and ML)/ML Models/\"\n",
    "\n",
    "ML_model_file_name = f'2022_05_11 rf_model {(category)}.joblib'\n",
    "rf_model = load(ML_model_file_path + ML_model_file_name) \n",
    "\n",
    "# CHANGE bigram_file_path\n",
    "bigram_file_path = \"C:/Users/tsaie/OneDrive/Desktop/000 Resumes & Projects/# Projects/FARS/Ester Tsai (Bigram and ML)/Bigrams/\"\n",
    "\n",
    "gold_bigram_file_name = f'{(category)} gold_bigrams.csv'\n",
    "gold_bigram_df = pd.read_csv(bigram_file_path + gold_bigram_file_name, index_col=0)\n",
    "display(gold_bigram_df.head(3))\n",
    "\n",
    "fake_bigram_file_name = f'{(category)} fake_bigrams.csv'\n",
    "fake_bigram_df = pd.read_csv(bigram_file_path + fake_bigram_file_name, index_col=0)\n",
    "display(fake_bigram_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(df):\n",
    "    \n",
    "    start_1 = time()\n",
    "    \n",
    "    # Removing emtpy cells\n",
    "    df.dropna(inplace=True)\n",
    "    df['review_cleaned'] = df['review_body'].copy()\n",
    "    \n",
    "    # Removing Unicode Chars (URL)\n",
    "    df['review_cleaned'] = df['review_cleaned'].apply(\n",
    "        lambda rev: re.sub(r\"(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", rev))\n",
    "        \n",
    "    # Replace HTML keywords with blank space (\"&quot;\", \"br\", \"&#34\")\n",
    "    remove_dict = {\"<br /><br />\": \" \", \"<br />\": \" \", \"br \": \"\", \"&quot;\": \" \", \"&#34\": \" \",\n",
    "                   \"<BR>\": \" \", \"_\": \"\"}\n",
    "    for key, val in remove_dict.items():\n",
    "        df['review_cleaned'] = df['review_cleaned'].apply(\n",
    "            lambda x: x.replace(key, val))\n",
    "        \n",
    "    end_1 = time()\n",
    "        \n",
    "    print(f\"\\n######## [{end_1 - start_1:0.2f} secs] Remove URL and HTML Keywords Complete ########\")\n",
    "    \n",
    "    start_2 = time()\n",
    "    \n",
    "    # Remove Punctuations and numbers\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    df['review_cleaned'] = df['review_cleaned'].apply(\n",
    "        lambda x: ' '.join([word for word in tokenizer.tokenize(x)]))\n",
    "    \n",
    "    remove_dict = {\"0\": \"\", \"1\": \"\", \"2\": \"\", \"3\": \"\", \"4\": \"\", \"5\": \"\", \"6\": \"\", \"7\": \"\", \"8\": \"\", \"9\": \"\",\n",
    "                   \"(\": \"\", \")\":\"\"}\n",
    "    for key, val in remove_dict.items():\n",
    "        df['review_cleaned'] = df['review_cleaned'].apply(\n",
    "            lambda x: x.replace(key, val))\n",
    "    \n",
    "    end_2 = time()\n",
    "    \n",
    "    print(f\"\\n######## [{end_2 - start_2:0.2f} secs] Remove Punctuation and Numbers Complete ########\")\n",
    "    \n",
    "    start_3 = time()\n",
    "    \n",
    "    # Lowercase Words\n",
    "    df['review_cleaned'] = df['review_cleaned'].str.lower()\n",
    "    \n",
    "    end_3 = time()\n",
    "    \n",
    "    print(f\"\\n######## [{end_3 - start_3:0.2f} secs] Lowercase Complete ########\")\n",
    "    \n",
    "    start_4 = time()\n",
    "\n",
    "    # Remove Stop Words.\n",
    "    stop = stopwords.words('english')\n",
    "      \n",
    "    df['review_cleaned'] = df['review_cleaned'].apply(\n",
    "        lambda x: ' '.join([word for word in x.split() if word.strip() not in stop]))\n",
    "    \n",
    "    end_4 = time()\n",
    "    \n",
    "    print(f\"\\n######## [{end_4 - start_4:0.2f} secs] Remove Stop Words Complete ########\")\n",
    "    \n",
    "    start_5 = time()\n",
    "    \n",
    "    # Lemmatization using .lemma_\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "    df['review_cleaned'] = df['review_cleaned'].apply(\n",
    "        lambda x: ' '.join([token.lemma_ for token in nlp(x)]))\n",
    "    \n",
    "    end_5 = time()\n",
    "    \n",
    "    print(f\"\\n######## [{end_5 - start_5:0.2f} secs] Lemmatization Complete ########\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "from itertools import chain\n",
    "\n",
    "def find_ngrams(input_list, n):\n",
    "    return list(zip(*[input_list[i:] for i in range(n)]))\n",
    "\n",
    "def add_bigram_column(df):\n",
    "    copy = df.copy()\n",
    "    copy['bigrams'] = copy['review_cleaned'].map(lambda x: find_ngrams(x.split(), 2))\n",
    "    return copy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_bigram_count(bigrams, bigram_dict):\n",
    "    if len(bigrams) == 0:\n",
    "        return 0\n",
    "    \n",
    "    count = 0\n",
    "    for bigram in bigrams:\n",
    "        if bigram in bigram_dict.keys():\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def get_count_percent(bigrams, bigram_dict):\n",
    "    if len(bigrams) == 0:\n",
    "        return 0\n",
    "    \n",
    "    count = get_bigram_count(bigrams, bigram_dict)\n",
    "    return count / len(bigrams)\n",
    "\n",
    "def get_averge_top_score(bigrams, bigram_dict, topN=5):\n",
    "    if len(bigrams) == 0:\n",
    "        return 0\n",
    "    \n",
    "    scores = np.array([])\n",
    "    for bigram in bigrams:\n",
    "        if bigram in bigram_dict.keys():\n",
    "            scores = np.append(scores, bigram_dict[bigram])\n",
    "    \n",
    "    if len(bigrams) <= topN:\n",
    "        return scores.mean()\n",
    "    \n",
    "    sort_descending = -np.sort(-scores)[:topN]\n",
    "    avg_top_score = sort_descending.mean()\n",
    "    return avg_top_score\n",
    "\n",
    "def get_normalized_score(bigrams, bigram_dict):\n",
    "    if len(bigrams) == 0:\n",
    "        return 0\n",
    "    \n",
    "    score = 0\n",
    "    for bigram in bigrams:\n",
    "        if bigram in bigram_dict.keys():\n",
    "            score += bigram_dict[bigram]\n",
    "    return score / len(bigrams)\n",
    "\n",
    "def get_unique_percent(bigrams, bigram_dict, the_other_bigram_dict, unique_threshold=0):\n",
    "    # Count the number of bigrams in a review that appear in bigram_dict but not the_other_bigram_dict. \n",
    "    # Can adjust unique_threshold so you can count also the bigrams that appear in ...\n",
    "    # ...the_other_bigram_dict fewer than unique_threshold times\n",
    "    \n",
    "    if len(bigrams) == 0:\n",
    "        return 0\n",
    "    \n",
    "    count = get_bigram_count(bigrams, bigram_dict)\n",
    "    for bigram in bigrams:\n",
    "        if bigram in the_other_bigram_dict.keys():\n",
    "            if the_other_bigram_dict[bigram] > unique_threshold:\n",
    "                count -= 1\n",
    "    return count / len(bigrams)\n",
    "\n",
    "def has_fake_keywords(bigrams):\n",
    "    fake_keywords = ['honest', 'unbiased', 'unbias', 'biased', 'bias', 'neutral', 'impartial', 'truthful', \n",
    "                     'discount', 'free', 'promotion', 'promote', 'complimentary', 'test', 'influence', 'influencer',\n",
    "                     'independent']\n",
    "    fake_tuples = [('receive', 'product'), ('product', 'receive'), ('provide', 'review'), \n",
    "                   ('product', 'exchange'), ('exchange', 'review'), ('review', 'opinion'),\n",
    "                   ('sample', 'provide'), ('provide', 'sample'), ('sample', 'review'), ('review', 'sample'), \n",
    "                   ('sample', 'product'), ('supply', 'sample'), ('receive', 'sample'), ('sample', 'receive')] \n",
    "    \n",
    "    for kw in fake_tuples:\n",
    "        if kw in bigrams:\n",
    "            return True\n",
    "            \n",
    "    for bigram in bigrams:\n",
    "        for kw in fake_keywords:\n",
    "            if kw in bigram:\n",
    "                return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def add_gold_fake_features(df):\n",
    "    \n",
    "    for gold_or_fake in ['gold', 'fake']:\n",
    "\n",
    "        exec(f\"df['{gold_or_fake}%'] = df['bigrams'].apply(\\\n",
    "            lambda x: get_count_percent(x, {gold_or_fake}_bigram_dict_filtered))\")\n",
    "        \n",
    "        exec(f\"df['{gold_or_fake}_unique%'] = df['bigrams'].apply(\\\n",
    "            lambda x: get_unique_percent(x, {gold_or_fake}_bigram_dict, {gold_or_fake}_bigram_dict, 3))\")\n",
    "\n",
    "        exec(f\"df['{gold_or_fake}_score'] = df['bigrams'].apply(\\\n",
    "            lambda x: get_normalized_score(x, {gold_or_fake}_bigram_dict))\")\n",
    "        \n",
    "        exec(f\"df['{gold_or_fake}_top_score'] = df['bigrams'].apply(\\\n",
    "            lambda x: get_averge_top_score(x, {gold_or_fake}_bigram_dict, 1))\")\n",
    "        \n",
    "        exec(f\"df['{gold_or_fake}_top_avg_score'] = df['bigrams'].apply(\\\n",
    "            lambda x: get_averge_top_score(x, {gold_or_fake}_bigram_dict, 5))\")\n",
    "\n",
    "    df['has_fake_keywords'] = df['bigrams'].apply(lambda x: has_fake_keywords(x))\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def gold_bigram_df_to_vars(gold_bigram_df):\n",
    "    global gold_bigrams, gold_bigram_dict, gold_bigram_dict_filtered\n",
    "    gold_bigrams = gold_bigram_df['bigram'].to_list()\n",
    "    str_to_tuple = lambda x: (x.split(\"'\")[1], x.split(\"'\")[3])\n",
    "    gold_bigrams = list(map(str_to_tuple, gold_bigrams))\n",
    "    gold_bigram_dict = {gold_bigrams[i]: gold_bigram_df['count'].iloc[i] for i in range(len(gold_bigrams))}\n",
    "    gold_bigram_dict_filtered = dict((k, v) for k, v in gold_bigram_dict.items() if v >= 2)\n",
    "    \n",
    "def fake_bigram_df_to_vars(fake_bigram_df):\n",
    "    global fake_bigrams, fake_bigram_dict, fake_bigram_dict_filtered\n",
    "    fake_bigrams = fake_bigram_df['bigram'].to_list()\n",
    "    str_to_tuple = lambda x: (x.split(\"'\")[1], x.split(\"'\")[3])\n",
    "    fake_bigrams = list(map(str_to_tuple, fake_bigrams))\n",
    "    fake_bigram_dict = {fake_bigrams[i]: fake_bigram_df['count'].iloc[i] for i in range(len(fake_bigrams))}\n",
    "    fake_bigram_dict_filtered = dict((k, v) for k, v in fake_bigram_dict.items() if v >= 2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def clean_review_and_add_features(review, gold_bigram_df, fake_bigram_df):\n",
    "    \n",
    "    # Create dataframe for the input review\n",
    "    df = pd.DataFrame(data={'review_body': review}, index=[0])\n",
    "    \n",
    "    # Create helper variables\n",
    "#     gold_bigram_df_to_vars(gold_bigram_df)\n",
    "#     fake_bigram_df_to_vars(fake_bigram_df)\n",
    "    \n",
    "    # Clean the user input\n",
    "    df = data_cleaning(df) \n",
    "    \n",
    "    # Add the column 'bigrams'\n",
    "    df['bigrams'] = df['review_cleaned'].map(lambda x: find_ngrams(x.split(), 2))\n",
    "    df['bigram_count'] = df['bigrams'].apply(lambda x: len(x))\n",
    "    \n",
    "    # Add features to score the user input\n",
    "    df = add_gold_fake_features(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create helper variables (TAKES ~20 SECONDS TO LOAD)\n",
    "gold_bigram_df_to_vars(gold_bigram_df)\n",
    "fake_bigram_df_to_vars(fake_bigram_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try changing the review and see what the RandomForest model predicts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######## [0.01 secs] Remove URL and HTML Keywords Complete ########\n",
      "\n",
      "######## [0.01 secs] Remove Punctuation and Numbers Complete ########\n",
      "\n",
      "######## [0.00 secs] Lowercase Complete ########\n",
      "\n",
      "######## [0.01 secs] Remove Stop Words Complete ########\n",
      "\n",
      "######## [0.98 secs] Lemmatization Complete ########\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_cleaned</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>bigram_count</th>\n",
       "      <th>gold%</th>\n",
       "      <th>gold_unique%</th>\n",
       "      <th>gold_score</th>\n",
       "      <th>gold_top_score</th>\n",
       "      <th>gold_top_avg_score</th>\n",
       "      <th>fake%</th>\n",
       "      <th>fake_unique%</th>\n",
       "      <th>fake_score</th>\n",
       "      <th>fake_top_score</th>\n",
       "      <th>fake_top_avg_score</th>\n",
       "      <th>has_fake_keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beauty Category: review received product in ex...</td>\n",
       "      <td>beauty category review receive product exchang...</td>\n",
       "      <td>[(beauty, category), (category, review), (revi...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>30.333333</td>\n",
       "      <td>141.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1195.555556</td>\n",
       "      <td>4237.0</td>\n",
       "      <td>2117.6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_body  \\\n",
       "0  Beauty Category: review received product in ex...   \n",
       "\n",
       "                                      review_cleaned  \\\n",
       "0  beauty category review receive product exchang...   \n",
       "\n",
       "                                             bigrams  bigram_count     gold%  \\\n",
       "0  [(beauty, category), (category, review), (revi...             9  0.777778   \n",
       "\n",
       "   gold_unique%  gold_score  gold_top_score  gold_top_avg_score     fake%  \\\n",
       "0      0.111111   30.333333           141.0                53.0  0.777778   \n",
       "\n",
       "   fake_unique%   fake_score  fake_top_score  fake_top_avg_score  \\\n",
       "0           0.0  1195.555556          4237.0              2117.6   \n",
       "\n",
       "   has_fake_keywords  \n",
       "0               True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Number of features of the model must match the input. Model n_features is 8 and input n_features is 12 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-238f748ddd53>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mdf_for_prediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprepare_df_for_prediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_input_processed_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mprediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprobabilities\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrf_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_for_prediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrf_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_for_prediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minterpret_prediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproba\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    627\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    628\u001b[0m         \"\"\"\n\u001b[1;32m--> 629\u001b[1;33m         \u001b[0mproba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    630\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    671\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m         \u001b[1;31m# Check data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 673\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    674\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m         \u001b[1;31m# Assign chunk of trees to jobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    419\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    394\u001b[0m         \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_features_\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 396\u001b[1;33m             raise ValueError(\"Number of features of the model must \"\n\u001b[0m\u001b[0;32m    397\u001b[0m                              \u001b[1;34m\"match the input. Model n_features is %s and \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m                              \u001b[1;34m\"input n_features is %s \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Number of features of the model must match the input. Model n_features is 8 and input n_features is 12 "
     ]
    }
   ],
   "source": [
    "review = \"Beauty Category: review received product in exchange for honest review of this beauty supply\"\n",
    "#9994 \"This is a thick high quality cable, my picture is crystal clear and this is just the right length for my needs, I received this in exchange for my honest review.\"\n",
    "#112 Small,lightweight,fits on my desk. Does not come with batteries,I already had some.I received a discount for this review.\n",
    "#59356 Turbo charger, small and portable and fits perfectly in your pocket! A nice boost over regular chargers. I received this in exchange for an unbiased review.\n",
    "\n",
    "# Clean the user input and create a dataframe for it\n",
    "user_input_processed_df = clean_review_and_add_features(review, gold_bigram_df, fake_bigram_df)\n",
    "display(user_input_processed_df)\n",
    "\n",
    "def prepare_df_for_prediction(processed_df):\n",
    "    features = ['bigram_count', \n",
    "            'fake%', 'fake_unique%', 'fake_score', 'fake_top_avg_score', 'fake_top_score',\n",
    "            'gold%', 'gold_unique%', 'gold_score', 'gold_top_avg_score', 'gold_top_score',\n",
    "            'has_fake_keywords']\n",
    "    return processed_df[features]\n",
    "\n",
    "df_for_prediction = prepare_df_for_prediction(user_input_processed_df)\n",
    "\n",
    "prediction, probabilities = rf_model.predict(df_for_prediction), rf_model.predict_proba(df_for_prediction)[0]\n",
    "\n",
    "def interpret_prediction(review, pred, proba):\n",
    "    prob_unverified, prob_verified = round(proba[0] * 100, 1), round(proba[1] * 100, 1)\n",
    "    print(f'\\nREVIEW: \"{review}\" \\n')\n",
    "    if pred == 1:\n",
    "        print(f'PREDICTION: VERIFIED ({prob_verified}%) | UNVERIFIED ({prob_unverified}%)')\n",
    "    if pred == 0:\n",
    "        print(f'PREDICTION: UNVERIFIED ({prob_unverified}%) | VERIFIED ({prob_verified}%)')\n",
    "\n",
    "interpret_prediction(review, prediction[0], probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gold & Fake Reviews from the Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Electronics\n",
    "\n",
    "### Gold:\n",
    "\n",
    "\n",
    "### Fake:\n",
    "\n",
    "Turbo charger, small and portable and fits perfectly in your pocket! A nice boost over regular chargers. I received this in exchange for an unbiased review.\n",
    "\n",
    "This is a thick high quality cable, my picture is crystal clear and this is just the right length for my needs, I received this in exchange for my honest review.\n",
    "\n",
    "Small, lightweight, fits on my desk. Does not come with batteries, I already had some. I received a discount for this review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beauty\n",
    "\n",
    "### Gold:\n",
    "\n",
    "\n",
    "### Fake:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toys\n",
    "\n",
    "### Gold:\n",
    "\n",
    "\n",
    "### Fake:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Office Products\n",
    "\n",
    "### Gold:\n",
    "\n",
    "\n",
    "### Fake:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apparel\n",
    "\n",
    "### Gold:\n",
    "\n",
    "\n",
    "### Fake:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
